
==> ÂÆ°ËÆ°Êó•Âøó <==
|-----------|------------------------------------------------------------------------|----------|------|---------|---------------------|---------------------|
|  Command  |                                  Args                                  | Profile  | User | Version |     Start Time      |      End Time       |
|-----------|------------------------------------------------------------------------|----------|------|---------|---------------------|---------------------|
| start     | --registry-mirror=https://registry.docker-cn.com                       | minikube | hdy  | v1.33.1 | 22 Aug 24 13:27 CST |                     |
|           | --image-mirror-country=cn                                              |          |      |         |                     |                     |
|           | --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers |          |      |         |                     |                     |
|           | --vm-driver=virtualbox --memory=4096 --cpus=6                          |          |      |         |                     |                     |
| start     | --registry-mirror=https://registry.docker-cn.com                       | minikube | hdy  | v1.33.1 | 22 Aug 24 13:28 CST |                     |
|           | --image-mirror-country=cn                                              |          |      |         |                     |                     |
|           | --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers |          |      |         |                     |                     |
|           | --vm-driver=virtualbox --memory=4096 --cpus=6                          |          |      |         |                     |                     |
| start     | --registry-mirror=https://registry.docker-cn.com                       | minikube | hdy  | v1.33.1 | 22 Aug 24 13:28 CST |                     |
|           | --image-mirror-country=cn                                              |          |      |         |                     |                     |
|           | --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers |          |      |         |                     |                     |
|           | --vm-driver=virtualbox --memory=4096 --cpus=6                          |          |      |         |                     |                     |
| start     | --registry-mirror=https://registry.docker-cn.com                       | minikube | hdy  | v1.33.1 | 22 Aug 24 13:29 CST |                     |
|           | --image-mirror-country=cn                                              |          |      |         |                     |                     |
|           | --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers |          |      |         |                     |                     |
|           | --vm-driver=virtualbox --memory=4096 --cpus=6                          |          |      |         |                     |                     |
| start     | --registry-mirror=https://registry.docker-cn.com                       | minikube | hdy  | v1.33.1 | 22 Aug 24 13:29 CST |                     |
|           | --image-mirror-country=cn                                              |          |      |         |                     |                     |
|           | --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers |          |      |         |                     |                     |
|           | --vm-driver=virtualbox --memory=4096 --cpus=6                          |          |      |         |                     |                     |
| config    |                                                                        | minikube | hdy  | v1.33.1 | 22 Aug 24 13:31 CST | 22 Aug 24 13:31 CST |
| config    | cpus                                                                   | minikube | hdy  | v1.33.1 | 22 Aug 24 13:31 CST | 22 Aug 24 13:31 CST |
| config    | get cpu                                                                | minikube | hdy  | v1.33.1 | 22 Aug 24 13:31 CST |                     |
| config    | get cpus                                                               | minikube | hdy  | v1.33.1 | 22 Aug 24 13:31 CST |                     |
| config    | get iso-url                                                            | minikube | hdy  | v1.33.1 | 22 Aug 24 13:41 CST |                     |
| config    | get iso-url                                                            | minikube | hdy  | v1.33.1 | 22 Aug 24 13:41 CST |                     |
| config    | get                                                                    | minikube | hdy  | v1.33.1 | 22 Aug 24 13:41 CST |                     |
| config    | get                                                                    | minikube | hdy  | v1.33.1 | 22 Aug 24 13:41 CST |                     |
| config    | list                                                                   | minikube | hdy  | v1.33.1 | 22 Aug 24 13:42 CST | 22 Aug 24 13:42 CST |
| config    | list                                                                   | minikube | hdy  | v1.33.1 | 22 Aug 24 13:42 CST | 22 Aug 24 13:42 CST |
| config    | list                                                                   | minikube | hdy  | v1.33.1 | 22 Aug 24 13:43 CST | 22 Aug 24 13:43 CST |
| config    | list                                                                   | minikube | hdy  | v1.33.1 | 22 Aug 24 13:43 CST | 22 Aug 24 13:43 CST |
| config    | view                                                                   | minikube | hdy  | v1.33.1 | 22 Aug 24 13:43 CST | 22 Aug 24 13:43 CST |
| config    | list                                                                   | minikube | hdy  | v1.33.1 | 22 Aug 24 13:44 CST | 22 Aug 24 13:44 CST |
| config    | list ios_log                                                           | minikube | hdy  | v1.33.1 | 22 Aug 24 13:44 CST | 22 Aug 24 13:44 CST |
| config    | list get log_dir                                                       | minikube | hdy  | v1.33.1 | 22 Aug 24 13:44 CST | 22 Aug 24 13:44 CST |
| config    | get log_dir                                                            | minikube | hdy  | v1.33.1 | 22 Aug 24 13:44 CST |                     |
| config    | get log_dir                                                            | minikube | hdy  | v1.33.1 | 22 Aug 24 13:44 CST |                     |
| start     | --registry-mirror=https://registry.docker-cn.com                       | minikube | hdy  | v1.33.1 | 22 Aug 24 14:27 CST |                     |
|           | --image-mirror-country=cn                                              |          |      |         |                     |                     |
|           | --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers |          |      |         |                     |                     |
|           | --vm-driver=virtualbox --memory=4096 --cpus=6                          |          |      |         |                     |                     |
| start     | --registry-mirror=https://registry.docker-cn.com                       | minikube | hdy  | v1.33.1 | 22 Aug 24 14:27 CST |                     |
|           | --image-mirror-country=cn                                              |          |      |         |                     |                     |
|           | --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers |          |      |         |                     |                     |
|           | --vm-driver=virtualbox --memory=4096 --cpus=6                          |          |      |         |                     |                     |
| start     | --registry-mirror=https://registry.docker-cn.com                       | minikube | hdy  | v1.33.1 | 22 Aug 24 14:29 CST |                     |
|           | --image-mirror-country=cn                                              |          |      |         |                     |                     |
|           | --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers |          |      |         |                     |                     |
|           | --vm-driver=virtualbox --memory=4096 --cpus=6                          |          |      |         |                     |                     |
| start     | --registry-mirror=https://registry.docker-cn.com                       | minikube | hdy  | v1.33.1 | 22 Aug 24 14:30 CST | 22 Aug 24 14:32 CST |
|           | --image-mirror-country=cn                                              |          |      |         |                     |                     |
|           | --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers |          |      |         |                     |                     |
|           | --vm-driver=virtualbox --memory=4096 --cpus=6                          |          |      |         |                     |                     |
| kubectl   | -- get pods -A                                                         | minikube | hdy  | v1.33.1 | 22 Aug 24 14:33 CST | 22 Aug 24 14:33 CST |
| kubectl   | -- get pods -A                                                         | minikube | hdy  | v1.33.1 | 22 Aug 24 14:35 CST | 22 Aug 24 14:35 CST |
| stop      |                                                                        | minikube | hdy  | v1.33.1 | 22 Aug 24 14:42 CST | 22 Aug 24 14:42 CST |
| start     |                                                                        | minikube | hdy  | v1.33.1 | 22 Aug 24 15:00 CST | 22 Aug 24 15:01 CST |
| config    | view                                                                   | minikube | hdy  | v1.33.1 | 22 Aug 24 15:04 CST | 22 Aug 24 15:04 CST |
| config    | view                                                                   | minikube | hdy  | v1.33.1 | 22 Aug 24 15:04 CST | 22 Aug 24 15:04 CST |
| dashboard | --url                                                                  | minikube | hdy  | v1.33.1 | 22 Aug 24 15:07 CST |                     |
| dashboard | --url                                                                  | minikube | hdy  | v1.33.1 | 22 Aug 24 15:08 CST |                     |
| dashboard | --url                                                                  | minikube | hdy  | v1.33.1 | 22 Aug 24 15:10 CST |                     |
| dashboard | --url                                                                  | minikube | hdy  | v1.33.1 | 22 Aug 24 15:10 CST |                     |
| dashboard |                                                                        | minikube | hdy  | v1.33.1 | 22 Aug 24 15:11 CST |                     |
| dashboard |                                                                        | minikube | hdy  | v1.33.1 | 22 Aug 24 15:11 CST |                     |
| dashboard |                                                                        | minikube | hdy  | v1.33.1 | 22 Aug 24 15:12 CST |                     |
| dashboard |                                                                        | minikube | hdy  | v1.33.1 | 22 Aug 24 15:13 CST |                     |
|-----------|------------------------------------------------------------------------|----------|------|---------|---------------------|---------------------|


==> ‰∏äÊ¨°ÂêØÂä® <==
Log file created at: 2024/08/22 15:00:24
Running on machine: hdy-thinkpad
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0822 15:00:24.252095 1999755 out.go:291] Setting OutFile to fd 1 ...
I0822 15:00:24.252339 1999755 out.go:343] isatty.IsTerminal(1) = true
I0822 15:00:24.252343 1999755 out.go:304] Setting ErrFile to fd 2...
I0822 15:00:24.252349 1999755 out.go:343] isatty.IsTerminal(2) = true
I0822 15:00:24.252612 1999755 root.go:338] Updating PATH: /home/hdy/.minikube/bin
W0822 15:00:24.252711 1999755 root.go:314] Error reading config file at /home/hdy/.minikube/config/config.json: open /home/hdy/.minikube/config/config.json: no such file or directory
I0822 15:00:24.253326 1999755 out.go:298] Setting JSON to false
I0822 15:00:24.274419 1999755 start.go:129] hostinfo: {"hostname":"hdy-thinkpad","uptime":1039466,"bootTime":1723270559,"procs":470,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.4.0-147-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"f621e81a-a3c3-4d8f-8178-168d67567c97"}
I0822 15:00:24.274585 1999755 start.go:139] virtualization: kvm host
I0822 15:00:24.279757 1999755 out.go:177] üòÑ  Ubuntu 20.04 ‰∏äÁöÑ minikube v1.33.1
W0822 15:00:24.281575 1999755 preload.go:294] Failed to list preload files: open /home/hdy/.minikube/cache/preloaded-tarball: no such file or directory
I0822 15:00:24.281650 1999755 notify.go:220] Checking for updates...
I0822 15:00:24.282938 1999755 config.go:182] Loaded profile config "minikube": Driver=virtualbox, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0822 15:00:24.283348 1999755 driver.go:392] Setting default libvirt URI to qemu:///system
I0822 15:00:24.323149 1999755 virtualbox.go:136] virtual box version: 6.1.38_Ubuntur153438
I0822 15:00:24.325224 1999755 out.go:177] ‚ú®  Ê†πÊçÆÁé∞ÊúâÁöÑÈÖçÁΩÆÊñá‰ª∂‰ΩøÁî® virtualbox È©±Âä®Á®ãÂ∫è
I0822 15:00:24.327618 1999755 start.go:297] selected driver: virtualbox
I0822 15:00:24.327630 1999755 start.go:901] validating driver "virtualbox" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.33.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4096 CPUs:6 DiskSize:20000 Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[https://registry.docker-cn.com] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.59.105 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hdy:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0822 15:00:24.327804 1999755 start.go:912] status for virtualbox: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:6.1.38_Ubuntur153438
}
I0822 15:00:24.330420 1999755 cni.go:84] Creating CNI manager for ""
I0822 15:00:24.330442 1999755 cni.go:158] "virtualbox" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0822 15:00:24.330534 1999755 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.33.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4096 CPUs:6 DiskSize:20000 Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[https://registry.docker-cn.com] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.59.105 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hdy:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0822 15:00:24.330687 1999755 iso.go:125] acquiring lock: {Name:mkd610ba99d06f1dfd4cfc5ed11a64b76b9766c0 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0822 15:00:24.332620 1999755 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0822 15:00:24.334385 1999755 profile.go:143] Saving config to /home/hdy/.minikube/profiles/minikube/config.json ...
I0822 15:00:24.334483 1999755 cache.go:107] acquiring lock: {Name:mk35524ded2d17b63e65d19fbd1c882c9a61af6e Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0822 15:00:24.334526 1999755 cache.go:107] acquiring lock: {Name:mk9a1f02cb36e6aa32a1ac8b6291aef7381dd7c8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0822 15:00:24.334543 1999755 cache.go:107] acquiring lock: {Name:mkafe9b8af63dbd48d910a7ce4551230b697c7e4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0822 15:00:24.334555 1999755 cache.go:107] acquiring lock: {Name:mkfa07ee598326a9fd68b7cf4c181d8e887690e1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0822 15:00:24.334616 1999755 cache.go:115] /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5 exists
I0822 15:00:24.334634 1999755 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5" -> "/home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5" took 176.148¬µs
I0822 15:00:24.334652 1999755 cache.go:115] /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.9 exists
I0822 15:00:24.334647 1999755 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5 -> /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner_v5 succeeded
I0822 15:00:24.334664 1999755 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9" -> "/home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.9" took 159.905¬µs
I0822 15:00:24.334654 1999755 cache.go:107] acquiring lock: {Name:mk3a13c05f638fa6abe1a00bcdfbb7d422be0924 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0822 15:00:24.334676 1999755 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9 -> /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/pause_3.9 succeeded
I0822 15:00:24.334676 1999755 cache.go:115] /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.30.0 exists
I0822 15:00:24.334688 1999755 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.30.0" -> "/home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.30.0" took 194.466¬µs
I0822 15:00:24.334705 1999755 cache.go:115] /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.30.0 exists
I0822 15:00:24.334698 1999755 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.30.0 -> /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler_v1.30.0 succeeded
I0822 15:00:24.334695 1999755 cache.go:107] acquiring lock: {Name:mk6538f1a5ebb7fddc594529d60fd25f5bf471df Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0822 15:00:24.334714 1999755 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.30.0" -> "/home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.30.0" took 171.609¬µs
I0822 15:00:24.334726 1999755 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.30.0 -> /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy_v1.30.0 succeeded
I0822 15:00:24.334726 1999755 cache.go:115] /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.30.0 exists
I0822 15:00:24.334703 1999755 cache.go:107] acquiring lock: {Name:mk7284c9a59e97ae1b6a23c60799b52c982ac95d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0822 15:00:24.334751 1999755 cache.go:115] /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.11.1 exists
I0822 15:00:24.334747 1999755 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.30.0" -> "/home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.30.0" took 96.192¬µs
I0822 15:00:24.334749 1999755 start.go:360] acquireMachinesLock for minikube: {Name:mk87858bf388ee461ed7b59f19b27e05897d2b41 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0822 15:00:24.334759 1999755 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.30.0 -> /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager_v1.30.0 succeeded
I0822 15:00:24.334761 1999755 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.1" -> "/home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.11.1" took 69.759¬µs
I0822 15:00:24.334776 1999755 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.1 -> /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/coredns_v1.11.1 succeeded
I0822 15:00:24.334760 1999755 cache.go:107] acquiring lock: {Name:mk2087793fe4c5bcf0a49d6a906f89b20c7f9a62 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0822 15:00:24.334814 1999755 start.go:364] duration metric: took 49.886¬µs to acquireMachinesLock for "minikube"
I0822 15:00:24.334819 1999755 cache.go:115] /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.12-0 exists
I0822 15:00:24.334826 1999755 start.go:96] Skipping create...Using existing machine configuration
I0822 15:00:24.334830 1999755 fix.go:54] fixHost starting: 
I0822 15:00:24.334830 1999755 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.12-0" -> "/home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.12-0" took 166.431¬µs
I0822 15:00:24.334834 1999755 cache.go:115] /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.30.0 exists
I0822 15:00:24.334841 1999755 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.12-0 -> /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/etcd_3.5.12-0 succeeded
I0822 15:00:24.334844 1999755 cache.go:96] cache image "registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.30.0" -> "/home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.30.0" took 124.216¬µs
I0822 15:00:24.334851 1999755 cache.go:80] save to tar file registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.30.0 -> /home/hdy/.minikube/cache/images/amd64/registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver_v1.30.0 succeeded
I0822 15:00:24.334864 1999755 cache.go:87] Successfully saved all images to host disk.
I0822 15:00:24.335081 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:00:24.759115 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="poweroff"
VMStateChangeTime="2024-08-22T06:42:52.000000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
}
I0822 15:00:24.759156 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:24.759273 1999755 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0822 15:00:24.759299 1999755 fix.go:138] unexpected machine state, will restart: <nil>
I0822 15:00:24.764754 1999755 out.go:177] üîÑ  Restarting existing virtualbox VM for "minikube" ...
I0822 15:00:24.766329 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:00:24.904271 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="poweroff"
VMStateChangeTime="2024-08-22T06:42:52.000000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
}
I0822 15:00:24.904301 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:24.904429 1999755 main.go:141] libmachine: Check network to re-create if needed...
I0822 15:00:24.904457 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage list hostonlyifs
I0822 15:00:25.075796 1999755 main.go:141] libmachine: STDOUT:
{
Name:            vboxnet0
GUID:            786f6276-656e-4074-8000-0a0027000000
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     fe80::800:27ff:fe00:0
IPV6NetworkMaskPrefixLength: 64
HardwareAddress: 0a:00:27:00:00:00
MediumType:      Ethernet
Wireless:        No
Status:          Up
VBoxNetworkName: HostInterfaceNetworking-vboxnet0

}
I0822 15:00:25.075826 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:25.076821 1999755 main.go:141] libmachine: Searching for hostonly interface for IPv4: 192.168.59.1 and Mask: ffffff00
I0822 15:00:25.076849 1999755 main.go:141] libmachine: Found: vboxnet0
I0822 15:00:25.076871 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage list dhcpservers
I0822 15:00:25.143526 1999755 main.go:141] libmachine: STDOUT:
{
NetworkName:    HostInterfaceNetworking-vboxnet0
Dhcpd IP:       192.168.59.13
LowerIPAddress: 192.168.59.100
UpperIPAddress: 192.168.59.254
NetworkMask:    255.255.255.0
Enabled:        Yes
Global Configuration:
    minLeaseTime:     default
    defaultLeaseTime: default
    maxLeaseTime:     default
    Forced options:   None
    Suppressed opts.: None
        1/legacy: 255.255.255.0
Groups:               None
Individual Configs:   None
}
I0822 15:00:25.143548 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:25.143639 1999755 main.go:141] libmachine: Removing orphan DHCP servers...
I0822 15:00:25.143667 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage list hostonlyifs
I0822 15:00:25.314105 1999755 main.go:141] libmachine: STDOUT:
{
Name:            vboxnet0
GUID:            786f6276-656e-4074-8000-0a0027000000
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     fe80::800:27ff:fe00:0
IPV6NetworkMaskPrefixLength: 64
HardwareAddress: 0a:00:27:00:00:00
MediumType:      Ethernet
Wireless:        No
Status:          Up
VBoxNetworkName: HostInterfaceNetworking-vboxnet0

}
I0822 15:00:25.314122 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:25.314216 1999755 main.go:141] libmachine: Adding/Modifying DHCP server "192.168.59.6" with address range "192.168.59.100" - "192.168.59.254"...
I0822 15:00:25.314231 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage list dhcpservers
I0822 15:00:25.383709 1999755 main.go:141] libmachine: STDOUT:
{
NetworkName:    HostInterfaceNetworking-vboxnet0
Dhcpd IP:       192.168.59.13
LowerIPAddress: 192.168.59.100
UpperIPAddress: 192.168.59.254
NetworkMask:    255.255.255.0
Enabled:        Yes
Global Configuration:
    minLeaseTime:     default
    defaultLeaseTime: default
    maxLeaseTime:     default
    Forced options:   None
    Suppressed opts.: None
        1/legacy: 255.255.255.0
Groups:               None
Individual Configs:   None
}
I0822 15:00:25.383732 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:25.383942 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage dhcpserver modify --netname HostInterfaceNetworking-vboxnet0 --ip 192.168.59.6 --netmask 255.255.255.0 --lowerip 192.168.59.100 --upperip 192.168.59.254 --enable
I0822 15:00:25.470675 1999755 main.go:141] libmachine: STDOUT:
{
}
I0822 15:00:25.470697 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:25.470733 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage modifyvm minikube --nic2 hostonly --nictype2 virtio --nicpromisc2 deny --hostonlyadapter2 vboxnet0 --cableconnected2 on
I0822 15:00:25.557191 1999755 main.go:141] libmachine: STDOUT:
{
}
I0822 15:00:25.557219 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:25.557561 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage modifyvm minikube --natpf1 delete ssh
I0822 15:00:25.632616 1999755 main.go:141] libmachine: STDOUT:
{
}
I0822 15:00:25.632633 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:25.632663 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage modifyvm minikube --natpf1 ssh,tcp,127.0.0.1,32911,,22
I0822 15:00:25.711669 1999755 main.go:141] libmachine: STDOUT:
{
}
I0822 15:00:25.711686 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:25.711707 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage startvm minikube --type headless
I0822 15:00:26.024348 1999755 main.go:141] libmachine: STDOUT:
{
Waiting for VM "minikube" to power on...
VM "minikube" has been successfully started.
}
I0822 15:00:26.024359 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:26.024383 1999755 main.go:141] libmachine: Checking vm logs: /home/hdy/.minikube/machines/minikube/minikube/Logs/VBox.log
I0822 15:00:26.024770 1999755 main.go:141] libmachine: Waiting for an IP...
I0822 15:00:26.024778 1999755 main.go:141] libmachine: Getting to WaitForSSH function...
I0822 15:00:26.024937 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:00:26.025133 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:00:26.025139 1999755 main.go:141] libmachine: About to run SSH command:
exit 0
I0822 15:00:57.047161 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0822 15:00:57.047187 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:00:57.168547 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:00:57.168577 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:57.168674 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:00:57.328797 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:00:57.328844 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:57.329250 1999755 main.go:141] libmachine: Host-only MAC: 0800277d220e

I0822 15:00:57.329800 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:00:57.330211 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:00:57.330227 1999755 main.go:141] libmachine: About to run SSH command:
ip addr show
I0822 15:00:57.427789 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:8c:bc:80 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 metric 1024 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86391sec preferred_lft 86391sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7d:22:0e brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.105/24 metric 1024 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 591sec preferred_lft 591sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I0822 15:00:57.427858 1999755 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:8c:bc:80 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 metric 1024 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86391sec preferred_lft 86391sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7d:22:0e brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.105/24 metric 1024 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 591sec preferred_lft 591sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I0822 15:00:57.427876 1999755 main.go:141] libmachine: IP is 192.168.59.105
I0822 15:00:57.427892 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:00:57.583873 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:00:57.583898 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:57.584010 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:00:57.736497 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:00:57.736538 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:57.736905 1999755 main.go:141] libmachine: Host-only MAC: 0800277d220e

I0822 15:00:57.737259 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:00:57.737760 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:00:57.737795 1999755 main.go:141] libmachine: About to run SSH command:
ip addr show
I0822 15:00:57.844517 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:8c:bc:80 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 metric 1024 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86390sec preferred_lft 86390sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7d:22:0e brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.105/24 metric 1024 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 590sec preferred_lft 590sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I0822 15:00:57.844597 1999755 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:8c:bc:80 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 metric 1024 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86390sec preferred_lft 86390sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7d:22:0e brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.105/24 metric 1024 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 590sec preferred_lft 590sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I0822 15:00:57.844625 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage list hostonlyifs
I0822 15:00:57.989779 1999755 main.go:141] libmachine: STDOUT:
{
Name:            vboxnet0
GUID:            786f6276-656e-4074-8000-0a0027000000
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     fe80::800:27ff:fe00:0
IPV6NetworkMaskPrefixLength: 64
HardwareAddress: 0a:00:27:00:00:00
MediumType:      Ethernet
Wireless:        No
Status:          Up
VBoxNetworkName: HostInterfaceNetworking-vboxnet0

}
I0822 15:00:57.989810 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:57.990823 1999755 main.go:141] libmachine: Found: vboxnet0
I0822 15:00:57.991475 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:00:58.073451 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:00:58.073473 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:58.073561 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:00:58.155604 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:00:58.155619 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:58.155798 1999755 main.go:141] libmachine: Host-only MAC: 0800277d220e

I0822 15:00:58.155979 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:00:58.156171 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:00:58.156178 1999755 main.go:141] libmachine: About to run SSH command:
ip addr show
I0822 15:00:58.226267 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:8c:bc:80 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 metric 1024 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86390sec preferred_lft 86390sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7d:22:0e brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.105/24 metric 1024 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 590sec preferred_lft 590sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I0822 15:00:58.226310 1999755 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:8c:bc:80 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 metric 1024 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86390sec preferred_lft 86390sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7d:22:0e brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.105/24 metric 1024 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 590sec preferred_lft 590sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I0822 15:00:58.226433 1999755 profile.go:143] Saving config to /home/hdy/.minikube/profiles/minikube/config.json ...
I0822 15:00:58.226738 1999755 machine.go:94] provisionDockerMachine start ...
I0822 15:00:58.226937 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:00:58.227147 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:00:58.227155 1999755 main.go:141] libmachine: About to run SSH command:
hostname
I0822 15:00:58.303680 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0822 15:00:58.303699 1999755 buildroot.go:166] provisioning hostname "minikube"
I0822 15:00:58.304028 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:00:58.304327 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:00:58.304342 1999755 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0822 15:00:58.435319 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0822 15:00:58.435534 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:00:58.435776 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:00:58.435791 1999755 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0822 15:00:58.546791 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0822 15:00:58.546812 1999755 buildroot.go:172] set auth options {CertDir:/home/hdy/.minikube CaCertPath:/home/hdy/.minikube/certs/ca.pem CaPrivateKeyPath:/home/hdy/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/hdy/.minikube/machines/server.pem ServerKeyPath:/home/hdy/.minikube/machines/server-key.pem ClientKeyPath:/home/hdy/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/hdy/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/hdy/.minikube}
I0822 15:00:58.546868 1999755 buildroot.go:174] setting up certificates
I0822 15:00:58.546878 1999755 provision.go:84] configureAuth start
I0822 15:00:58.546913 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:00:58.712391 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:00:58.712471 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:58.712690 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:00:58.872344 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:00:58.872366 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:00:58.872552 1999755 main.go:141] libmachine: Host-only MAC: 0800277d220e

I0822 15:00:58.872827 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:00:58.873063 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:00:58.873073 1999755 main.go:141] libmachine: About to run SSH command:
ip addr show
I0822 15:00:58.976159 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:8c:bc:80 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 metric 1024 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86389sec preferred_lft 86389sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7d:22:0e brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.105/24 metric 1024 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 589sec preferred_lft 589sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I0822 15:00:58.976220 1999755 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:8c:bc:80 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 metric 1024 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86389sec preferred_lft 86389sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7d:22:0e brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.105/24 metric 1024 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 589sec preferred_lft 589sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I0822 15:00:58.976234 1999755 provision.go:143] copyHostCerts
I0822 15:00:58.976290 1999755 exec_runner.go:144] found /home/hdy/.minikube/ca.pem, removing ...
I0822 15:00:58.976307 1999755 exec_runner.go:203] rm: /home/hdy/.minikube/ca.pem
I0822 15:00:58.976387 1999755 exec_runner.go:151] cp: /home/hdy/.minikube/certs/ca.pem --> /home/hdy/.minikube/ca.pem (1070 bytes)
I0822 15:00:58.976557 1999755 exec_runner.go:144] found /home/hdy/.minikube/cert.pem, removing ...
I0822 15:00:58.976563 1999755 exec_runner.go:203] rm: /home/hdy/.minikube/cert.pem
I0822 15:00:58.976604 1999755 exec_runner.go:151] cp: /home/hdy/.minikube/certs/cert.pem --> /home/hdy/.minikube/cert.pem (1115 bytes)
I0822 15:00:58.976710 1999755 exec_runner.go:144] found /home/hdy/.minikube/key.pem, removing ...
I0822 15:00:58.976715 1999755 exec_runner.go:203] rm: /home/hdy/.minikube/key.pem
I0822 15:00:58.976753 1999755 exec_runner.go:151] cp: /home/hdy/.minikube/certs/key.pem --> /home/hdy/.minikube/key.pem (1675 bytes)
I0822 15:00:58.976834 1999755 provision.go:117] generating server cert: /home/hdy/.minikube/machines/server.pem ca-key=/home/hdy/.minikube/certs/ca.pem private-key=/home/hdy/.minikube/certs/ca-key.pem org=hdy.minikube san=[127.0.0.1 192.168.59.105 localhost minikube]
I0822 15:00:59.376172 1999755 provision.go:177] copyRemoteCerts
I0822 15:00:59.376337 1999755 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0822 15:00:59.376348 1999755 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32911 SSHKeyPath:/home/hdy/.minikube/machines/minikube/id_rsa Username:docker}
I0822 15:00:59.441212 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0822 15:00:59.499712 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/machines/server.pem --> /etc/docker/server.pem (1172 bytes)
I0822 15:00:59.550024 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0822 15:00:59.610815 1999755 provision.go:87] duration metric: took 1.063920519s to configureAuth
I0822 15:00:59.610841 1999755 buildroot.go:189] setting minikube options for container-runtime
I0822 15:00:59.611232 1999755 config.go:182] Loaded profile config "minikube": Driver=virtualbox, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0822 15:00:59.611601 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:00:59.611939 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:00:59.611955 1999755 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0822 15:00:59.715869 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0822 15:00:59.715888 1999755 buildroot.go:70] root file system type: tmpfs
I0822 15:00:59.716053 1999755 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0822 15:00:59.716325 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:00:59.716596 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:00:59.716752 1999755 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=virtualbox --insecure-registry 10.96.0.0/12 --registry-mirror https://registry.docker-cn.com 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0822 15:00:59.844991 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=virtualbox --insecure-registry 10.96.0.0/12 --registry-mirror https://registry.docker-cn.com 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0822 15:00:59.845184 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:00:59.845389 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:00:59.845412 1999755 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0822 15:01:02.087997 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service ‚Üí /usr/lib/systemd/system/docker.service.

I0822 15:01:02.088017 1999755 machine.go:97] duration metric: took 3.861269852s to provisionDockerMachine
I0822 15:01:02.088033 1999755 start.go:293] postStartSetup for "minikube" (driver="virtualbox")
I0822 15:01:02.088049 1999755 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0822 15:01:02.088349 1999755 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0822 15:01:02.088363 1999755 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32911 SSHKeyPath:/home/hdy/.minikube/machines/minikube/id_rsa Username:docker}
I0822 15:01:02.160426 1999755 ssh_runner.go:195] Run: cat /etc/os-release
I0822 15:01:02.169359 1999755 info.go:137] Remote host: Buildroot 2023.02.9
I0822 15:01:02.169375 1999755 filesync.go:126] Scanning /home/hdy/.minikube/addons for local assets ...
I0822 15:01:02.169460 1999755 filesync.go:126] Scanning /home/hdy/.minikube/files for local assets ...
I0822 15:01:02.169503 1999755 start.go:296] duration metric: took 81.461269ms for postStartSetup
I0822 15:01:02.169531 1999755 fix.go:56] duration metric: took 37.834699114s for fixHost
I0822 15:01:02.169842 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:01:02.170182 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:01:02.170196 1999755 main.go:141] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I0822 15:01:02.269802 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: 1724310062.267803746

I0822 15:01:02.269820 1999755 fix.go:216] guest clock: 1724310062.267803746
I0822 15:01:02.269837 1999755 fix.go:229] Guest: 2024-08-22 15:01:02.267803746 +0800 CST Remote: 2024-08-22 15:01:02.16953578 +0800 CST m=+37.961362358 (delta=98.267966ms)
I0822 15:01:02.269883 1999755 fix.go:200] guest clock delta is within tolerance: 98.267966ms
I0822 15:01:02.269891 1999755 start.go:83] releasing machines lock for "minikube", held for 37.9350689s
I0822 15:01:02.269934 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:01:02.425350 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:01:02.425384 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:01:02.425523 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:01:02.564054 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:01:02.564092 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:01:02.564625 1999755 main.go:141] libmachine: Host-only MAC: 0800277d220e

I0822 15:01:02.565078 1999755 main.go:141] libmachine: Using SSH client type: native
I0822 15:01:02.565535 1999755 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32911 <nil> <nil>}
I0822 15:01:02.565556 1999755 main.go:141] libmachine: About to run SSH command:
ip addr show
I0822 15:01:02.674942 1999755 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:8c:bc:80 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 metric 1024 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86385sec preferred_lft 86385sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7d:22:0e brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.105/24 metric 1024 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 585sec preferred_lft 585sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:78:ed:0c:21 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever

I0822 15:01:02.675043 1999755 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:8c:bc:80 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 metric 1024 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86385sec preferred_lft 86385sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7d:22:0e brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.105/24 metric 1024 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 585sec preferred_lft 585sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:78:ed:0c:21 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever

END SSH

I0822 15:01:02.676394 1999755 ssh_runner.go:195] Run: cat /version.json
I0822 15:01:02.676410 1999755 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32911 SSHKeyPath:/home/hdy/.minikube/machines/minikube/id_rsa Username:docker}
I0822 15:01:02.676488 1999755 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.cn-hangzhou.aliyuncs.com/google_containers/
I0822 15:01:02.676536 1999755 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32911 SSHKeyPath:/home/hdy/.minikube/machines/minikube/id_rsa Username:docker}
I0822 15:01:02.723252 1999755 ssh_runner.go:195] Run: systemctl --version
I0822 15:01:03.161309 1999755 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0822 15:01:03.174048 1999755 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0822 15:01:03.174321 1999755 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0822 15:01:03.209834 1999755 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0822 15:01:03.209854 1999755 start.go:494] detecting cgroup driver to use...
I0822 15:01:03.209987 1999755 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0822 15:01:03.253994 1999755 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9"|' /etc/containerd/config.toml"
I0822 15:01:03.276883 1999755 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0822 15:01:03.297089 1999755 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0822 15:01:03.297493 1999755 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0822 15:01:03.318015 1999755 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0822 15:01:03.338542 1999755 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0822 15:01:03.361194 1999755 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0822 15:01:03.381838 1999755 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0822 15:01:03.403214 1999755 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0822 15:01:03.419607 1999755 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0822 15:01:03.441172 1999755 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0822 15:01:03.461007 1999755 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0822 15:01:03.481601 1999755 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0822 15:01:03.500425 1999755 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0822 15:01:03.673331 1999755 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0822 15:01:03.705397 1999755 start.go:494] detecting cgroup driver to use...
I0822 15:01:03.705626 1999755 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0822 15:01:03.729988 1999755 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0822 15:01:03.759723 1999755 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0822 15:01:03.785511 1999755 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0822 15:01:03.808167 1999755 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0822 15:01:03.836895 1999755 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0822 15:01:03.881358 1999755 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0822 15:01:03.904392 1999755 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0822 15:01:03.943230 1999755 ssh_runner.go:195] Run: which cri-dockerd
I0822 15:01:03.952327 1999755 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0822 15:01:03.973334 1999755 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (225 bytes)
I0822 15:01:04.013754 1999755 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0822 15:01:04.194444 1999755 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0822 15:01:04.374080 1999755 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0822 15:01:04.374190 1999755 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0822 15:01:04.410408 1999755 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0822 15:01:04.586061 1999755 ssh_runner.go:195] Run: sudo systemctl restart docker
I0822 15:01:07.228754 1999755 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.642667598s)
I0822 15:01:07.229031 1999755 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0822 15:01:07.261309 1999755 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0822 15:01:07.284825 1999755 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0822 15:01:07.461762 1999755 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0822 15:01:07.634778 1999755 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0822 15:01:07.803509 1999755 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0822 15:01:07.833953 1999755 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0822 15:01:07.858044 1999755 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0822 15:01:08.045672 1999755 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0822 15:01:08.183531 1999755 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0822 15:01:08.183978 1999755 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0822 15:01:08.193353 1999755 start.go:562] Will wait 60s for crictl version
I0822 15:01:08.193543 1999755 ssh_runner.go:195] Run: which crictl
I0822 15:01:08.198407 1999755 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0822 15:01:08.259710 1999755 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.2
RuntimeApiVersion:  v1
I0822 15:01:08.260109 1999755 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0822 15:01:08.307864 1999755 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0822 15:01:08.354029 1999755 out.go:204] üê≥  Ê≠£Âú® Docker 26.0.2 ‰∏≠ÂáÜÂ§á Kubernetes v1.30.0‚Ä¶
I0822 15:01:08.626913 1999755 ssh_runner.go:195] Run: grep 192.168.59.1	host.minikube.internal$ /etc/hosts
I0822 15:01:08.637382 1999755 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.59.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0822 15:01:08.660769 1999755 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.33.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4096 CPUs:6 DiskSize:20000 Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[https://registry.docker-cn.com] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.59.105 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hdy:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0822 15:01:08.661000 1999755 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0822 15:01:08.896564 1999755 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0822 15:01:08.897064 1999755 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0822 15:01:08.928133 1999755 docker.go:685] Got preloaded images: -- stdout --
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.12-0
registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.1
registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9
registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5

-- /stdout --
I0822 15:01:08.928153 1999755 docker.go:615] Images already preloaded, skipping extraction
I0822 15:01:08.928478 1999755 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0822 15:01:08.965370 1999755 docker.go:685] Got preloaded images: -- stdout --
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.30.0
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.12-0
registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.1
registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9
registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5

-- /stdout --
I0822 15:01:08.965385 1999755 cache_images.go:84] Images are preloaded, skipping loading
I0822 15:01:08.965393 1999755 kubeadm.go:928] updating node { 192.168.59.105 8443 v1.30.0 docker true true} ...
I0822 15:01:08.965584 1999755 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.59.105

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0822 15:01:08.965914 1999755 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0822 15:01:09.016907 1999755 cni.go:84] Creating CNI manager for ""
I0822 15:01:09.016950 1999755 cni.go:158] "virtualbox" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0822 15:01:09.016961 1999755 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0822 15:01:09.016986 1999755 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.59.105 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.59.105"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.59.105 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0822 15:01:09.017228 1999755 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.59.105
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.59.105
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.59.105"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0822 15:01:09.017518 1999755 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0822 15:01:09.036106 1999755 binaries.go:44] Found k8s binaries, skipping transfer
I0822 15:01:09.036303 1999755 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0822 15:01:09.055082 1999755 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (309 bytes)
I0822 15:01:09.100698 1999755 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0822 15:01:09.134583 1999755 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2225 bytes)
I0822 15:01:09.174096 1999755 ssh_runner.go:195] Run: grep 192.168.59.105	control-plane.minikube.internal$ /etc/hosts
I0822 15:01:09.182788 1999755 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.59.105	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0822 15:01:09.207674 1999755 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0822 15:01:09.394640 1999755 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0822 15:01:09.425947 1999755 certs.go:68] Setting up /home/hdy/.minikube/profiles/minikube for IP: 192.168.59.105
I0822 15:01:09.425966 1999755 certs.go:194] generating shared ca certs ...
I0822 15:01:09.425995 1999755 certs.go:226] acquiring lock for ca certs: {Name:mk66ab96664d1c183bcef6596d0a2ce356b8f97a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0822 15:01:09.426221 1999755 certs.go:235] skipping valid "minikubeCA" ca cert: /home/hdy/.minikube/ca.key
I0822 15:01:09.426287 1999755 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/hdy/.minikube/proxy-client-ca.key
I0822 15:01:09.426298 1999755 certs.go:256] generating profile certs ...
I0822 15:01:09.426421 1999755 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/hdy/.minikube/profiles/minikube/client.key
I0822 15:01:09.426515 1999755 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/hdy/.minikube/profiles/minikube/apiserver.key.7928cf1a
I0822 15:01:09.426585 1999755 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/hdy/.minikube/profiles/minikube/proxy-client.key
I0822 15:01:09.426801 1999755 certs.go:484] found cert: /home/hdy/.minikube/certs/ca-key.pem (1675 bytes)
I0822 15:01:09.426875 1999755 certs.go:484] found cert: /home/hdy/.minikube/certs/ca.pem (1070 bytes)
I0822 15:01:09.426920 1999755 certs.go:484] found cert: /home/hdy/.minikube/certs/cert.pem (1115 bytes)
I0822 15:01:09.426965 1999755 certs.go:484] found cert: /home/hdy/.minikube/certs/key.pem (1675 bytes)
I0822 15:01:09.428044 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0822 15:01:09.494531 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0822 15:01:09.549749 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0822 15:01:09.606451 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0822 15:01:09.662389 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0822 15:01:09.711576 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0822 15:01:09.772730 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0822 15:01:09.819637 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0822 15:01:09.869670 1999755 ssh_runner.go:362] scp /home/hdy/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0822 15:01:09.928668 1999755 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0822 15:01:09.966977 1999755 ssh_runner.go:195] Run: openssl version
I0822 15:01:09.978721 1999755 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0822 15:01:09.998475 1999755 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0822 15:01:10.008167 1999755 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Aug 22 06:32 /usr/share/ca-certificates/minikubeCA.pem
I0822 15:01:10.008615 1999755 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0822 15:01:10.020369 1999755 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0822 15:01:10.041009 1999755 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0822 15:01:10.051581 1999755 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0822 15:01:10.065535 1999755 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0822 15:01:10.077815 1999755 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0822 15:01:10.093400 1999755 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0822 15:01:10.105797 1999755 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0822 15:01:10.119570 1999755 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0822 15:01:10.132857 1999755 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.33.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4096 CPUs:6 DiskSize:20000 Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[https://registry.docker-cn.com] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository:registry.cn-hangzhou.aliyuncs.com/google_containers LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.59.105 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hdy:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0822 15:01:10.133293 1999755 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0822 15:01:10.178249 1999755 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0822 15:01:10.197568 1999755 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0822 15:01:10.197593 1999755 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0822 15:01:10.197619 1999755 kubeadm.go:587] restartPrimaryControlPlane start ...
I0822 15:01:10.197966 1999755 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0822 15:01:10.221778 1999755 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0822 15:01:10.222494 1999755 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /home/hdy/.kube/config
I0822 15:01:10.222696 1999755 kubeconfig.go:62] /home/hdy/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0822 15:01:10.223234 1999755 lock.go:35] WriteFile acquiring /home/hdy/.kube/config: {Name:mkb7a283322e15ed2632c3ffc704d2af9f11ba24 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0822 15:01:10.225663 1999755 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0822 15:01:10.245551 1999755 kubeadm.go:624] The running cluster does not require reconfiguration: 192.168.59.105
I0822 15:01:10.245600 1999755 kubeadm.go:1154] stopping kube-system containers ...
I0822 15:01:10.245917 1999755 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0822 15:01:10.284818 1999755 docker.go:483] Stopping containers: [4e1e89a7ccce 3ec9744779d0 74ef327036f5 623115b7506c 47f79a273098 f5ca09df6c55 a91d75cf5982 fe8a27b5c0f4 5b6c9357fcc1 1fc4f6a483bc d4d80f7bf621 2d209653031f 6b5386fc8173 8e77cfd9c70d ff93745d9859 adbfd7375cea 3f4d89093cf3]
I0822 15:01:10.285099 1999755 ssh_runner.go:195] Run: docker stop 4e1e89a7ccce 3ec9744779d0 74ef327036f5 623115b7506c 47f79a273098 f5ca09df6c55 a91d75cf5982 fe8a27b5c0f4 5b6c9357fcc1 1fc4f6a483bc d4d80f7bf621 2d209653031f 6b5386fc8173 8e77cfd9c70d ff93745d9859 adbfd7375cea 3f4d89093cf3
I0822 15:01:10.321195 1999755 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0822 15:01:10.357808 1999755 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0822 15:01:10.376350 1999755 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0822 15:01:10.376369 1999755 kubeadm.go:156] found existing configuration files:

I0822 15:01:10.376695 1999755 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0822 15:01:10.390873 1999755 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0822 15:01:10.391169 1999755 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0822 15:01:10.414545 1999755 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0822 15:01:10.433417 1999755 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0822 15:01:10.433655 1999755 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0822 15:01:10.449483 1999755 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0822 15:01:10.468630 1999755 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0822 15:01:10.468969 1999755 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0822 15:01:10.489301 1999755 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0822 15:01:10.509533 1999755 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0822 15:01:10.509805 1999755 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0822 15:01:10.529422 1999755 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0822 15:01:10.548495 1999755 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0822 15:01:10.759465 1999755 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0822 15:01:12.886634 1999755 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (2.127127004s)
I0822 15:01:12.886660 1999755 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0822 15:01:13.187389 1999755 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0822 15:01:13.277370 1999755 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0822 15:01:13.404649 1999755 api_server.go:52] waiting for apiserver process to appear ...
I0822 15:01:13.404864 1999755 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0822 15:01:13.905332 1999755 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0822 15:01:14.405335 1999755 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0822 15:01:14.448571 1999755 api_server.go:72] duration metric: took 1.043920792s to wait for apiserver process to appear ...
I0822 15:01:14.448593 1999755 api_server.go:88] waiting for apiserver healthz status ...
I0822 15:01:14.448616 1999755 api_server.go:253] Checking apiserver healthz at https://192.168.59.105:8443/healthz ...
I0822 15:01:14.449196 1999755 api_server.go:269] stopped: https://192.168.59.105:8443/healthz: Get "https://192.168.59.105:8443/healthz": dial tcp 192.168.59.105:8443: connect: connection refused
I0822 15:01:14.948712 1999755 api_server.go:253] Checking apiserver healthz at https://192.168.59.105:8443/healthz ...
I0822 15:01:17.262862 1999755 api_server.go:279] https://192.168.59.105:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W0822 15:01:17.262883 1999755 api_server.go:103] status: https://192.168.59.105:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I0822 15:01:17.262901 1999755 api_server.go:253] Checking apiserver healthz at https://192.168.59.105:8443/healthz ...
I0822 15:01:17.272876 1999755 api_server.go:279] https://192.168.59.105:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[-]autoregister-completion failed: reason withheld
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0822 15:01:17.272898 1999755 api_server.go:103] status: https://192.168.59.105:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[-]autoregister-completion failed: reason withheld
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0822 15:01:17.449530 1999755 api_server.go:253] Checking apiserver healthz at https://192.168.59.105:8443/healthz ...
I0822 15:01:17.462725 1999755 api_server.go:279] https://192.168.59.105:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0822 15:01:17.462750 1999755 api_server.go:103] status: https://192.168.59.105:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0822 15:01:17.949624 1999755 api_server.go:253] Checking apiserver healthz at https://192.168.59.105:8443/healthz ...
I0822 15:01:17.960072 1999755 api_server.go:279] https://192.168.59.105:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0822 15:01:17.960098 1999755 api_server.go:103] status: https://192.168.59.105:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0822 15:01:18.448719 1999755 api_server.go:253] Checking apiserver healthz at https://192.168.59.105:8443/healthz ...
I0822 15:01:18.457911 1999755 api_server.go:279] https://192.168.59.105:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0822 15:01:18.457943 1999755 api_server.go:103] status: https://192.168.59.105:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0822 15:01:18.948742 1999755 api_server.go:253] Checking apiserver healthz at https://192.168.59.105:8443/healthz ...
I0822 15:01:18.960836 1999755 api_server.go:279] https://192.168.59.105:8443/healthz returned 200:
ok
I0822 15:01:18.980490 1999755 api_server.go:141] control plane version: v1.30.0
I0822 15:01:18.980524 1999755 api_server.go:131] duration metric: took 4.531922478s to wait for apiserver health ...
I0822 15:01:18.980539 1999755 cni.go:84] Creating CNI manager for ""
I0822 15:01:18.980562 1999755 cni.go:158] "virtualbox" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0822 15:01:18.984975 1999755 out.go:177] üîó  ÈÖçÁΩÆ bridge CNI (Container Networking Interface) ...
I0822 15:01:18.986979 1999755 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0822 15:01:19.012585 1999755 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0822 15:01:19.048119 1999755 system_pods.go:43] waiting for kube-system pods to appear ...
I0822 15:01:19.072610 1999755 system_pods.go:59] 8 kube-system pods found
I0822 15:01:19.072656 1999755 system_pods.go:61] "coredns-7c445c467-hpct7" [8ef19b2b-eeec-4af3-9e1a-4e2d6b6fe89e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0822 15:01:19.072664 1999755 system_pods.go:61] "coredns-7c445c467-v9w6q" [9daf5f09-acea-42ec-bbf0-615bfe2c8356] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0822 15:01:19.072670 1999755 system_pods.go:61] "etcd-minikube" [57c043b6-af08-4fdc-baf3-e8b4b8a80b81] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0822 15:01:19.072676 1999755 system_pods.go:61] "kube-apiserver-minikube" [80dd9db6-297d-4e2f-81bb-ef09e9ffb63e] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0822 15:01:19.072681 1999755 system_pods.go:61] "kube-controller-manager-minikube" [454f1c9e-483d-4cab-8cfe-7713c3c8a6d8] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0822 15:01:19.072686 1999755 system_pods.go:61] "kube-proxy-w29jc" [e6454420-dac2-421e-9280-444140927684] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0822 15:01:19.072691 1999755 system_pods.go:61] "kube-scheduler-minikube" [5e143166-23fd-428a-820f-cd00109acdb6] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0822 15:01:19.072699 1999755 system_pods.go:61] "storage-provisioner" [d0cb4238-ec10-40f3-ac8b-4a43474e9a66] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0822 15:01:19.072705 1999755 system_pods.go:74] duration metric: took 24.573388ms to wait for pod list to return data ...
I0822 15:01:19.072711 1999755 node_conditions.go:102] verifying NodePressure condition ...
I0822 15:01:19.080854 1999755 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I0822 15:01:19.080870 1999755 node_conditions.go:123] node cpu capacity is 6
I0822 15:01:19.080879 1999755 node_conditions.go:105] duration metric: took 8.163822ms to run NodePressure ...
I0822 15:01:19.080894 1999755 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0822 15:01:19.483303 1999755 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0822 15:01:19.514129 1999755 ops.go:34] apiserver oom_adj: -16
I0822 15:01:19.514150 1999755 kubeadm.go:591] duration metric: took 9.316519352s to restartPrimaryControlPlane
I0822 15:01:19.514167 1999755 kubeadm.go:393] duration metric: took 9.381320751s to StartCluster
I0822 15:01:19.514194 1999755 settings.go:142] acquiring lock: {Name:mkc5ed08de06dc8f668563948ea0081cbb1a62b0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0822 15:01:19.514291 1999755 settings.go:150] Updating kubeconfig:  /home/hdy/.kube/config
I0822 15:01:19.515760 1999755 lock.go:35] WriteFile acquiring /home/hdy/.kube/config: {Name:mkb7a283322e15ed2632c3ffc704d2af9f11ba24 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0822 15:01:19.516107 1999755 global.go:112] Querying for installed drivers using PATH=/home/hdy/.minikube/bin:/home/hdy/data/gitHome/vue/bin:/home/hdy/data/gitHome/nginx/bin:/home/hdy/data/gitHome/java/bin:/home/hdy/data/gitHome/html/bin:/home/hdy/data/gitHome/go/bin:/home/hdy/data/gitHome/cpp/bin:/home/hdy/data/gitHome/python/bin:/home/hdy/data/gitHome/c/bin:/home/hdy/data/gitHome/mysql/bin:/home/hdy/data/gitHome/redis/bin:/home/hdy/data/gitHome/git/bin:/home/hdy/data/gitHome/re/bin:/home/hdy/data/gitHome/php/bin:/home/hdy/data/gitHome/k8s/bin:/home/hdy/data/gitHome/docker/bin:/home/hdy/data/gitHome/shell/bin:/home/hdy/data/gitHome/shell:/usr/local/hive/apache-hive-3.1.3-bin/bin:/home/hdy/qtcreator-5.0.3/bin:/usr/local/go-1.20.5/bin:/var/www/html/go/bin:/usr/local/apache-maven-3.8.6/bin:/usr/local/node-v18.16/bin:/usr/local/jdk/jdk-17.0.2/bin:/home/hdy/.local/bin:/home/hdy/bin:/home/hdy/data/gitHome/mysql/bin:/home/hdy/data/gitHome/redis/bin:/home/hdy/data/gitHome/git/bin:/home/hdy/data/gitHome/re/bin:/home/hdy/data/gitHome/php/bin:/home/hdy/data/gitHome/docker/bin:/home/hdy/data/gitHome/shell/bin:/home/hdy/data/gitHome/shell:/usr/local/hive/apache-hive-3.1.3-bin/bin:/home/hdy/qtcreator-5.0.3/bin:/usr/local/go-1.20.5/bin:/var/www/html/go/bin:/usr/local/apache-maven-3.8.6/bin:/usr/local/node-v18.16/bin:/usr/local/jdk/jdk-17.0.2/bin:/home/hdy/data/gitHome/mysql/bin:/home/hdy/data/gitHome/redis/bin:/home/hdy/data/gitHome/git/bin:/home/hdy/data/gitHome/re/bin:/home/hdy/data/gitHome/php/bin:/home/hdy/data/gitHome/docker/bin:/home/hdy/data/gitHome/shell/bin:/home/hdy/data/gitHome/shell:/usr/local/hive/apache-hive-3.1.3-bin/bin:/home/hdy/qtcreator-5.0.3/bin:/usr/local/go-1.20.5/bin:/var/www/html/go/bin:/usr/local/apache-maven-3.8.6/bin:/usr/local/node-v18.16/bin:/usr/local/jdk/jdk-17.0.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/hdy/.dotnet/tools:/usr/bin/hdy:/home/hdy/Ê°åÈù¢/soft/idea-IU-221.6008.13/bin:/home/hdy/.yarn/bin:/usr/local/hadoop/hadoop-3.3.6/bin:/usr/local/hadoop/hadoop-3.3.6/sbin:/home/hdy/data/note/0-wen/0-batch/shell:/home/hdy/data/note/0-wen/0-batch/Á≥ªÁªüËÑöÊú¨:/home/hdy/data/note/0-wen/0-batch/Á≥ªÁªüÈÖçÁΩÆ:/home/hdy/.dotnet/tools:/usr/bin/hdy:/home/hdy/Ê°åÈù¢/soft/idea-IU-221.6008.13/bin:/home/hdy/.yarn/bin:/usr/local/hadoop/hadoop-3.3.6/bin:/usr/local/hadoop/hadoop-3.3.6/sbin:/home/hdy/data/note/0-wen/0-batch/shell:/home/hdy/data/note/0-wen/0-batch/Á≥ªÁªüËÑöÊú¨:/home/hdy/data/note/0-wen/0-batch/Á≥ªÁªüÈÖçÁΩÆ:/home/hdy/.dotnet/tools:/usr/bin/hdy:/home/hdy/Ê°åÈù¢/soft/idea-IU-221.6008.13/bin:/home/hdy/.yarn/bin:/usr/local/hadoop/hadoop-3.3.6/bin:/usr/local/hadoop/hadoop-3.3.6/sbin:/home/hdy/data/note/0-wen/0-batch/shell:/home/hdy/data/note/0-wen/0-batch/Á≥ªÁªüËÑöÊú¨:/home/hdy/data/note/0-wen/0-batch/Á≥ªÁªüÈÖçÁΩÆ
I0822 15:01:19.516144 1999755 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0822 15:01:19.516179 1999755 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0822 15:01:19.516247 1999755 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0822 15:01:19.516258 1999755 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0822 15:01:19.516282 1999755 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0822 15:01:19.516285 1999755 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0822 15:01:19.516292 1999755 addons.go:243] addon storage-provisioner should already be in state true
I0822 15:01:19.516320 1999755 host.go:66] Checking if "minikube" exists ...
I0822 15:01:19.516385 1999755 config.go:182] Loaded profile config "minikube": Driver=virtualbox, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0822 15:01:19.516622 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:01:19.517082 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:01:19.586045 1999755 global.go:133] kvm2 default: true priority: 8, state: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0822 15:01:19.586486 1999755 global.go:133] qemu2 default: true priority: 7, state: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0822 15:01:19.630192 1999755 virtualbox.go:136] virtual box version: 6.1.38_Ubuntur153438
I0822 15:01:19.630215 1999755 global.go:133] virtualbox default: true priority: 6, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:6.1.38_Ubuntur153438
}
I0822 15:01:19.630569 1999755 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0822 15:01:19.637792 1999755 out.go:177] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                                   [31m‚îÇ[0m
[31m‚îÇ[0m    You have selected "virtualbox" driver, but there are better options !                          [31m‚îÇ[0m
[31m‚îÇ[0m    For better performance and support consider using a different driver:                          [31m‚îÇ[0m
[31m‚îÇ[0m            - kvm2                                                                                 [31m‚îÇ[0m
[31m‚îÇ[0m            - qemu2                                                                                [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                                   [31m‚îÇ[0m
[31m‚îÇ[0m    To turn off this warning run:                                                                  [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                                   [31m‚îÇ[0m
[31m‚îÇ[0m            $ minikube config set WantVirtualBoxDriverWarning false                                [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                                   [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                                   [31m‚îÇ[0m
[31m‚îÇ[0m    To learn more about on minikube drivers checkout https://minikube.sigs.k8s.io/docs/drivers/    [31m‚îÇ[0m
[31m‚îÇ[0m    To see benchmarks checkout https://minikube.sigs.k8s.io/docs/benchmarks/cpuusage/              [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                                   [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I0822 15:01:19.637633 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:01:19.637884 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:01:19.639463 1999755 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.59.105 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0822 15:01:19.637722 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:01:19.643323 1999755 out.go:177]     ‚ñ™ Ê≠£Âú®‰ΩøÁî®ÈïúÂÉè registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5
I0822 15:01:19.640977 1999755 out.go:177] üîé  Ê≠£Âú®È™åËØÅ Kubernetes ÁªÑ‰ª∂...
I0822 15:01:19.641021 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:01:19.645219 1999755 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0822 15:01:19.645234 1999755 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0822 15:01:19.645253 1999755 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2708 bytes)
I0822 15:01:19.645297 1999755 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32911 SSHKeyPath:/home/hdy/.minikube/machines/minikube/id_rsa Username:docker}
I0822 15:01:19.645590 1999755 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0822 15:01:19.645601 1999755 addons.go:243] addon default-storageclass should already be in state true
I0822 15:01:19.645627 1999755 host.go:66] Checking if "minikube" exists ...
I0822 15:01:19.646582 1999755 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I0822 15:01:19.763010 1999755 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
CfgFile="/home/hdy/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/hdy/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/hdy/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="74f20b14-8e1d-4462-9cd3-61919ce8b9d5"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=6
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-08-22T07:00:26.008000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/hdy/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="983f3d4d-ae6b-40d2-966f-18c45a4102b7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/hdy/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="89a74197-9477-4274-a63b-f1254b8065f1"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="0800278CBC80"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,32911,,22"
hostonlyadapter2="vboxnet0"
macaddress2="0800277D220E"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/hdy/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1724310047571
GuestAdditionsFacility_VirtualBox System Service=50,1724310048531
GuestAdditionsFacility_Seamless Mode=0,1724310047568
GuestAdditionsFacility_Graphics Mode=0,1724310047568
}
I0822 15:01:19.763031 1999755 main.go:141] libmachine: STDERR:
{
}
I0822 15:01:19.763143 1999755 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0822 15:01:19.763150 1999755 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0822 15:01:19.763164 1999755 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32911 SSHKeyPath:/home/hdy/.minikube/machines/minikube/id_rsa Username:docker}
I0822 15:01:19.918280 1999755 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0822 15:01:19.946542 1999755 api_server.go:52] waiting for apiserver process to appear ...
I0822 15:01:19.946813 1999755 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0822 15:01:19.955498 1999755 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0822 15:01:19.986079 1999755 api_server.go:72] duration metric: took 346.551186ms to wait for apiserver process to appear ...
I0822 15:01:19.986098 1999755 api_server.go:88] waiting for apiserver healthz status ...
I0822 15:01:19.986119 1999755 api_server.go:253] Checking apiserver healthz at https://192.168.59.105:8443/healthz ...
I0822 15:01:19.999311 1999755 api_server.go:279] https://192.168.59.105:8443/healthz returned 200:
ok
I0822 15:01:20.002132 1999755 api_server.go:141] control plane version: v1.30.0
I0822 15:01:20.002163 1999755 api_server.go:131] duration metric: took 16.045538ms to wait for apiserver health ...
I0822 15:01:20.002169 1999755 system_pods.go:43] waiting for kube-system pods to appear ...
I0822 15:01:20.025515 1999755 system_pods.go:59] 8 kube-system pods found
I0822 15:01:20.025538 1999755 system_pods.go:61] "coredns-7c445c467-hpct7" [8ef19b2b-eeec-4af3-9e1a-4e2d6b6fe89e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0822 15:01:20.025546 1999755 system_pods.go:61] "coredns-7c445c467-v9w6q" [9daf5f09-acea-42ec-bbf0-615bfe2c8356] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0822 15:01:20.025553 1999755 system_pods.go:61] "etcd-minikube" [57c043b6-af08-4fdc-baf3-e8b4b8a80b81] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0822 15:01:20.025558 1999755 system_pods.go:61] "kube-apiserver-minikube" [80dd9db6-297d-4e2f-81bb-ef09e9ffb63e] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0822 15:01:20.025564 1999755 system_pods.go:61] "kube-controller-manager-minikube" [454f1c9e-483d-4cab-8cfe-7713c3c8a6d8] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0822 15:01:20.025568 1999755 system_pods.go:61] "kube-proxy-w29jc" [e6454420-dac2-421e-9280-444140927684] Running
I0822 15:01:20.025572 1999755 system_pods.go:61] "kube-scheduler-minikube" [5e143166-23fd-428a-820f-cd00109acdb6] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0822 15:01:20.025575 1999755 system_pods.go:61] "storage-provisioner" [d0cb4238-ec10-40f3-ac8b-4a43474e9a66] Running
I0822 15:01:20.025580 1999755 system_pods.go:74] duration metric: took 23.405766ms to wait for pod list to return data ...
I0822 15:01:20.025588 1999755 kubeadm.go:576] duration metric: took 386.08542ms to wait for: map[apiserver:true system_pods:true]
I0822 15:01:20.025601 1999755 node_conditions.go:102] verifying NodePressure condition ...
I0822 15:01:20.032067 1999755 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I0822 15:01:20.032082 1999755 node_conditions.go:123] node cpu capacity is 6
I0822 15:01:20.032091 1999755 node_conditions.go:105] duration metric: took 6.485892ms to run NodePressure ...
I0822 15:01:20.032100 1999755 start.go:240] waiting for startup goroutines ...
I0822 15:01:20.059964 1999755 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0822 15:01:21.082410 1999755 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.126886218s)
I0822 15:01:21.082461 1999755 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.022475948s)
I0822 15:01:21.098535 1999755 out.go:177] üåü  ÂêØÁî®Êèí‰ª∂Ôºö storage-provisioner, default-storageclass
I0822 15:01:21.099998 1999755 addons.go:505] duration metric: took 1.583835524s for enable addons: enabled=[storage-provisioner default-storageclass]
I0822 15:01:21.100048 1999755 start.go:245] waiting for cluster config update ...
I0822 15:01:21.100065 1999755 start.go:254] writing updated cluster config ...
I0822 15:01:21.100597 1999755 ssh_runner.go:195] Run: rm -f paused
I0822 15:01:21.157729 1999755 start.go:600] kubectl: 1.28.4, cluster: 1.30.0 (minor skew: 2)
I0822 15:01:21.159288 1999755 out.go:177] 
W0822 15:01:21.160729 1999755 out.go:239] ‚ùó  /usr/bin/kubectl ÁöÑÁâàÊú¨‰∏∫ 1.28.4ÔºåÂèØËÉΩ‰∏é Kubernetes 1.30.0 ‰∏çÂÖºÂÆπ„ÄÇ
I0822 15:01:21.162086 1999755 out.go:177]     ‚ñ™ ÊÉ≥Ë¶Å‰ΩøÁî® kubectl v1.30.0 ÂêóÔºüÂ∞ùËØï‰ΩøÁî® 'minikube kubectl -- get pods -A' ÂëΩ‰ª§
I0822 15:01:21.164557 1999755 out.go:177] üèÑ  ÂÆåÊàêÔºÅkubectl Áé∞Âú®Â∑≤ÈÖçÁΩÆÔºåÈªòËÆ§‰ΩøÁî®"minikube"ÈõÜÁæ§Âíå"default"ÂëΩÂêçÁ©∫Èó¥


==> Docker <==
Aug 22 07:08:42 minikube dockerd[1024]: time="2024-08-22T07:08:42.962163115Z" level=info msg="Download failed, retrying (1/5): dial tcp 59.188.250.54:443: i/o timeout"
Aug 22 07:08:49 minikube cri-dockerd[1254]: time="2024-08-22T07:08:49Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Retrying in 1 second "
Aug 22 07:08:59 minikube cri-dockerd[1254]: time="2024-08-22T07:08:59Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Retrying in 1 second "
Aug 22 07:09:09 minikube cri-dockerd[1254]: time="2024-08-22T07:09:09Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Retrying in 1 second "
Aug 22 07:09:19 minikube cri-dockerd[1254]: time="2024-08-22T07:09:19Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Retrying in 1 second "
Aug 22 07:09:29 minikube cri-dockerd[1254]: time="2024-08-22T07:09:29Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Retrying in 1 second "
Aug 22 07:09:39 minikube cri-dockerd[1254]: time="2024-08-22T07:09:39Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Retrying in 1 second "
Aug 22 07:09:49 minikube dockerd[1024]: time="2024-08-22T07:09:49.723406005Z" level=info msg="Download failed, retrying (2/5): dial tcp 59.188.250.54:443: i/o timeout"
Aug 22 07:09:49 minikube cri-dockerd[1254]: time="2024-08-22T07:09:49Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: 8e052fd7e2d0: Retrying in 10 seconds "
Aug 22 07:09:50 minikube dockerd[1024]: time="2024-08-22T07:09:50.198864267Z" level=info msg="Download failed, retrying (2/5): dial tcp 59.188.250.54:443: i/o timeout"
Aug 22 07:09:59 minikube cri-dockerd[1254]: time="2024-08-22T07:09:59Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Retrying in 1 second "
Aug 22 07:10:09 minikube cri-dockerd[1254]: time="2024-08-22T07:10:09Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Retrying in 1 second "
Aug 22 07:10:19 minikube cri-dockerd[1254]: time="2024-08-22T07:10:19Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Retrying in 1 second "
Aug 22 07:10:22 minikube dockerd[1024]: time="2024-08-22T07:10:22.335483273Z" level=error msg="Not continuing with pull after error: error pulling image configuration: download failed after attempts=6: dial tcp 59.188.250.54:443: i/o timeout" spanID=9539c77b41a5ab63 traceID=3734b07446fd7545a2b4207cc7abf346
Aug 22 07:10:22 minikube cri-dockerd[1254]: time="2024-08-22T07:10:22Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Retrying in 1 second "
Aug 22 07:10:37 minikube dockerd[1024]: time="2024-08-22T07:10:37.357440953Z" level=warning msg="Error getting v2 registry: Get \"https://registry.docker-cn.com/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" spanID=191047f1e9c0e220 traceID=1218a8422a1e82cd46ffc7bb0e94d0be
Aug 22 07:10:37 minikube dockerd[1024]: time="2024-08-22T07:10:37.357833998Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.docker-cn.com/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" spanID=191047f1e9c0e220 traceID=1218a8422a1e82cd46ffc7bb0e94d0be
Aug 22 07:10:39 minikube dockerd[1024]: time="2024-08-22T07:10:39.494227178Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" spanID=191047f1e9c0e220 traceID=1218a8422a1e82cd46ffc7bb0e94d0be
Aug 22 07:10:52 minikube cri-dockerd[1254]: time="2024-08-22T07:10:52Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Aug 22 07:11:02 minikube cri-dockerd[1254]: time="2024-08-22T07:11:02Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Aug 22 07:11:12 minikube cri-dockerd[1254]: time="2024-08-22T07:11:12Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Aug 22 07:11:22 minikube cri-dockerd[1254]: time="2024-08-22T07:11:22Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Aug 22 07:11:32 minikube cri-dockerd[1254]: time="2024-08-22T07:11:32Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Aug 22 07:11:42 minikube cri-dockerd[1254]: time="2024-08-22T07:11:42Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Aug 22 07:11:44 minikube dockerd[1024]: time="2024-08-22T07:11:44.918117803Z" level=info msg="Download failed, retrying (1/5): dial tcp 199.59.148.7:443: i/o timeout"
Aug 22 07:11:45 minikube dockerd[1024]: time="2024-08-22T07:11:45.536676849Z" level=info msg="Download failed, retrying (1/5): dial tcp 199.59.148.7:443: i/o timeout"
Aug 22 07:11:52 minikube cri-dockerd[1254]: time="2024-08-22T07:11:52Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Aug 22 07:12:02 minikube cri-dockerd[1254]: time="2024-08-22T07:12:02Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Aug 22 07:12:12 minikube cri-dockerd[1254]: time="2024-08-22T07:12:12Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Aug 22 07:12:22 minikube cri-dockerd[1254]: time="2024-08-22T07:12:22Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Aug 22 07:12:32 minikube cri-dockerd[1254]: time="2024-08-22T07:12:32Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Aug 22 07:12:42 minikube cri-dockerd[1254]: time="2024-08-22T07:12:42Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:49 minikube dockerd[1024]: 2024/08/22 07:12:49 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:50 minikube dockerd[1024]: 2024/08/22 07:12:50 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:50 minikube dockerd[1024]: 2024/08/22 07:12:50 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:50 minikube dockerd[1024]: 2024/08/22 07:12:50 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:50 minikube dockerd[1024]: 2024/08/22 07:12:50 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Aug 22 07:12:51 minikube dockerd[1024]: time="2024-08-22T07:12:51.633523425Z" level=info msg="Download failed, retrying (2/5): dial tcp 174.37.154.236:443: i/o timeout"
Aug 22 07:12:52 minikube cri-dockerd[1254]: time="2024-08-22T07:12:52Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Retrying in 10 seconds "
Aug 22 07:12:53 minikube dockerd[1024]: time="2024-08-22T07:12:53.084928496Z" level=info msg="Download failed, retrying (2/5): dial tcp 174.37.154.236:443: i/o timeout"
Aug 22 07:13:02 minikube cri-dockerd[1254]: time="2024-08-22T07:13:02Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Aug 22 07:13:12 minikube cri-dockerd[1254]: time="2024-08-22T07:13:12Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Aug 22 07:13:21 minikube dockerd[1024]: time="2024-08-22T07:13:21.408302692Z" level=error msg="Not continuing with pull after error: error pulling image configuration: download failed after attempts=6: dial tcp 174.37.154.236:443: i/o timeout" spanID=191047f1e9c0e220 traceID=1218a8422a1e82cd46ffc7bb0e94d0be
Aug 22 07:13:21 minikube cri-dockerd[1254]: time="2024-08-22T07:13:21Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Aug 22 07:13:36 minikube dockerd[1024]: time="2024-08-22T07:13:36.820062881Z" level=warning msg="Error getting v2 registry: Get \"https://registry.docker-cn.com/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" spanID=b57dd459f0e803a3 traceID=f6e796bb8642fbdfb6a61b32475255b3
Aug 22 07:13:36 minikube dockerd[1024]: time="2024-08-22T07:13:36.820293180Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.docker-cn.com/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" spanID=b57dd459f0e803a3 traceID=f6e796bb8642fbdfb6a61b32475255b3
Aug 22 07:13:37 minikube dockerd[1024]: time="2024-08-22T07:13:37.642197752Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" spanID=b57dd459f0e803a3 traceID=f6e796bb8642fbdfb6a61b32475255b3
Aug 22 07:13:49 minikube cri-dockerd[1254]: time="2024-08-22T07:13:49Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: 8e052fd7e2d0: Pulling fs layer "
Aug 22 07:13:59 minikube cri-dockerd[1254]: time="2024-08-22T07:13:59Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: 8e052fd7e2d0: Pulling fs layer "


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
1074a830568ca       6e38f40d628db       12 minutes ago      Running             storage-provisioner       3                   15c58d2ed423b       storage-provisioner
414cfbdcd3092       cbb01a7bd410d       12 minutes ago      Running             coredns                   1                   72b65fc634a60       coredns-7c445c467-hpct7
ed942fc66d536       cbb01a7bd410d       12 minutes ago      Running             coredns                   1                   3f3ec9313cf17       coredns-7c445c467-v9w6q
66b9676eddf9e       6e38f40d628db       12 minutes ago      Exited              storage-provisioner       2                   15c58d2ed423b       storage-provisioner
2c39b6cf5b7d1       a0bf559e280cf       12 minutes ago      Running             kube-proxy                1                   d208934cde63f       kube-proxy-w29jc
e8745f9c8c4a1       c7aad43836fa5       12 minutes ago      Running             kube-controller-manager   1                   cb033c55d4988       kube-controller-manager-minikube
feaa13ef46a50       c42f13656d0b2       12 minutes ago      Running             kube-apiserver            1                   4a706aaf5e4a2       kube-apiserver-minikube
487f1bf369e3a       3861cfcd7c04c       12 minutes ago      Running             etcd                      1                   1598d9bfaca90       etcd-minikube
f5fea639ca6e9       259c8277fcbbc       12 minutes ago      Running             kube-scheduler            1                   6802467b6845b       kube-scheduler-minikube
3ec9744779d0e       cbb01a7bd410d       41 minutes ago      Exited              coredns                   0                   47f79a2730986       coredns-7c445c467-hpct7
74ef327036f57       cbb01a7bd410d       41 minutes ago      Exited              coredns                   0                   f5ca09df6c55a       coredns-7c445c467-v9w6q
623115b7506cf       a0bf559e280cf       41 minutes ago      Exited              kube-proxy                0                   fe8a27b5c0f4b       kube-proxy-w29jc
1fc4f6a483bca       3861cfcd7c04c       41 minutes ago      Exited              etcd                      0                   3f4d89093cf3e       etcd-minikube
d4d80f7bf6216       c7aad43836fa5       41 minutes ago      Exited              kube-controller-manager   0                   adbfd7375cea0       kube-controller-manager-minikube
2d209653031fa       c42f13656d0b2       41 minutes ago      Exited              kube-apiserver            0                   ff93745d9859b       kube-apiserver-minikube
6b5386fc81732       259c8277fcbbc       41 minutes ago      Exited              kube-scheduler            0                   8e77cfd9c70dc       kube-scheduler-minikube


==> coredns [3ec9744779d0] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [414cfbdcd309] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2


==> coredns [74ef327036f5] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [ed942fc66d53] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_08_22T14_32_37_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 22 Aug 2024 06:32:33 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 22 Aug 2024 07:14:02 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 22 Aug 2024 07:11:40 +0000   Thu, 22 Aug 2024 06:32:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 22 Aug 2024 07:11:40 +0000   Thu, 22 Aug 2024 06:32:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 22 Aug 2024 07:11:40 +0000   Thu, 22 Aug 2024 06:32:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 22 Aug 2024 07:11:40 +0000   Thu, 22 Aug 2024 07:01:27 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.59.105
  Hostname:    minikube
Capacity:
  cpu:                6
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             4010248Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             4010248Ki
  pods:               110
System Info:
  Machine ID:                 3291cd46635d424db328697011ad6c4f
  System UUID:                140bf274-1d8e-6244-9cd3-61919ce8b9d5
  Boot ID:                    1e17c1cb-b669-4279-8aeb-cc083b89953e
  Kernel Version:             5.10.207
  OS Image:                   Buildroot 2023.02.9
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.0.2
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-7c445c467-hpct7                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     41m
  kube-system                 coredns-7c445c467-v9w6q                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     41m
  kube-system                 etcd-minikube                                100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         41m
  kube-system                 kube-apiserver-minikube                      250m (4%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         41m
  kube-system                 kube-controller-manager-minikube             200m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         41m
  kube-system                 kube-proxy-w29jc                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         41m
  kube-system                 kube-scheduler-minikube                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         41m
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         41m
  kubernetes-dashboard        dashboard-metrics-scraper-b5fc48f67-h6pzr    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m48s
  kubernetes-dashboard        kubernetes-dashboard-779776cb65-wctlm        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m48s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (14%!)(MISSING)  0 (0%!)(MISSING)
  memory             240Mi (6%!)(MISSING)  340Mi (8%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 41m                kube-proxy       
  Normal  Starting                 12m                kube-proxy       
  Normal  NodeHasSufficientPID     41m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  41m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  41m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    41m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  Starting                 41m                kubelet          Starting kubelet.
  Normal  NodeReady                41m                kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           41m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 12m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  12m (x8 over 12m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    12m (x8 over 12m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     12m (x7 over 12m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  12m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           12m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Aug22 07:00] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000000] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000001] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +0.191165]  #2
[  +0.001993]  #3
[  +0.001939]  #4
[  +0.001068]  #5
[  +0.023222] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.
[  +4.507087] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[ +10.753099] systemd-fstab-generator[224]: Ignoring "noauto" option for root device
[  +2.342156] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000018] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000003] NFSD: Unable to initialize client recovery tracking! (-2)
[ +10.481300] systemd-fstab-generator[668]: Ignoring "noauto" option for root device
[  +0.158652] systemd-fstab-generator[680]: Ignoring "noauto" option for root device
[Aug22 07:01] systemd-fstab-generator[948]: Ignoring "noauto" option for root device
[  +0.522035] systemd-fstab-generator[990]: Ignoring "noauto" option for root device
[  +0.175191] systemd-fstab-generator[1001]: Ignoring "noauto" option for root device
[  +0.210431] systemd-fstab-generator[1015]: Ignoring "noauto" option for root device
[  +2.368077] kauditd_printk_skb: 149 callbacks suppressed
[  +0.507883] systemd-fstab-generator[1207]: Ignoring "noauto" option for root device
[  +0.177823] systemd-fstab-generator[1219]: Ignoring "noauto" option for root device
[  +0.167068] systemd-fstab-generator[1231]: Ignoring "noauto" option for root device
[  +0.240281] systemd-fstab-generator[1246]: Ignoring "noauto" option for root device
[  +1.352998] systemd-fstab-generator[1379]: Ignoring "noauto" option for root device
[  +3.782604] systemd-fstab-generator[1520]: Ignoring "noauto" option for root device
[  +0.092235] kauditd_printk_skb: 129 callbacks suppressed
[  +5.331862] kauditd_printk_skb: 80 callbacks suppressed
[  +1.254225] systemd-fstab-generator[2249]: Ignoring "noauto" option for root device
[  +5.454408] kauditd_printk_skb: 34 callbacks suppressed
[  +5.076757] kauditd_printk_skb: 28 callbacks suppressed
[Aug22 07:10] kauditd_printk_skb: 20 callbacks suppressed


==> etcd [1fc4f6a483bc] <==
{"level":"warn","ts":"2024-08-22T06:32:30.185652Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-08-22T06:32:30.185827Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.59.105:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.59.105:2380","--initial-cluster=minikube=https://192.168.59.105:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.59.105:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.59.105:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-08-22T06:32:30.186238Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-08-22T06:32:30.186273Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.59.105:2380"]}
{"level":"info","ts":"2024-08-22T06:32:30.186339Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-08-22T06:32:30.188029Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.59.105:2379"]}
{"level":"info","ts":"2024-08-22T06:32:30.188428Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":6,"max-cpu-available":6,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.59.105:2380"],"listen-peer-urls":["https://192.168.59.105:2380"],"advertise-client-urls":["https://192.168.59.105:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.59.105:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.59.105:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-08-22T06:32:30.191988Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.621009ms"}
{"level":"info","ts":"2024-08-22T06:32:30.199773Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"24d1819b191cc3b5","cluster-id":"5a9a8e760cd75cc6"}
{"level":"info","ts":"2024-08-22T06:32:30.199931Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 switched to configuration voters=()"}
{"level":"info","ts":"2024-08-22T06:32:30.199975Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 became follower at term 0"}
{"level":"info","ts":"2024-08-22T06:32:30.199994Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 24d1819b191cc3b5 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-08-22T06:32:30.200022Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 became follower at term 1"}
{"level":"info","ts":"2024-08-22T06:32:30.200156Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 switched to configuration voters=(2653044158639162293)"}
{"level":"warn","ts":"2024-08-22T06:32:30.205411Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-08-22T06:32:30.210758Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-08-22T06:32:30.212728Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-08-22T06:32:30.216271Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"24d1819b191cc3b5","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-08-22T06:32:30.22239Z","caller":"etcdserver/server.go:744","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"24d1819b191cc3b5","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-08-22T06:32:30.222785Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-22T06:32:30.222909Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-22T06:32:30.222935Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-22T06:32:30.225551Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 switched to configuration voters=(2653044158639162293)"}
{"level":"info","ts":"2024-08-22T06:32:30.227524Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"5a9a8e760cd75cc6","local-member-id":"24d1819b191cc3b5","added-peer-id":"24d1819b191cc3b5","added-peer-peer-urls":["https://192.168.59.105:2380"]}
{"level":"info","ts":"2024-08-22T06:32:30.230932Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-08-22T06:32:30.231434Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"24d1819b191cc3b5","initial-advertise-peer-urls":["https://192.168.59.105:2380"],"listen-peer-urls":["https://192.168.59.105:2380"],"advertise-client-urls":["https://192.168.59.105:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.59.105:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-08-22T06:32:30.231521Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-08-22T06:32:30.231798Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.59.105:2380"}
{"level":"info","ts":"2024-08-22T06:32:30.231886Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.59.105:2380"}
{"level":"info","ts":"2024-08-22T06:32:30.603177Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 is starting a new election at term 1"}
{"level":"info","ts":"2024-08-22T06:32:30.603267Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 became pre-candidate at term 1"}
{"level":"info","ts":"2024-08-22T06:32:30.603302Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 received MsgPreVoteResp from 24d1819b191cc3b5 at term 1"}
{"level":"info","ts":"2024-08-22T06:32:30.603329Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 became candidate at term 2"}
{"level":"info","ts":"2024-08-22T06:32:30.603351Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 received MsgVoteResp from 24d1819b191cc3b5 at term 2"}
{"level":"info","ts":"2024-08-22T06:32:30.603376Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 became leader at term 2"}
{"level":"info","ts":"2024-08-22T06:32:30.603427Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 24d1819b191cc3b5 elected leader 24d1819b191cc3b5 at term 2"}
{"level":"info","ts":"2024-08-22T06:32:30.609404Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"24d1819b191cc3b5","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.59.105:2379]}","request-path":"/0/members/24d1819b191cc3b5/attributes","cluster-id":"5a9a8e760cd75cc6","publish-timeout":"7s"}
{"level":"info","ts":"2024-08-22T06:32:30.609698Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-22T06:32:30.610239Z","caller":"etcdserver/server.go:2578","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-22T06:32:30.610675Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-22T06:32:30.610966Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"5a9a8e760cd75cc6","local-member-id":"24d1819b191cc3b5","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-22T06:32:30.618428Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-22T06:32:30.618744Z","caller":"etcdserver/server.go:2602","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-22T06:32:30.619177Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-08-22T06:32:30.619228Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-08-22T06:32:30.620382Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-08-22T06:32:30.622401Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.59.105:2379"}
{"level":"info","ts":"2024-08-22T06:42:31.452739Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":605}
{"level":"info","ts":"2024-08-22T06:42:31.458174Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":605,"took":"4.888354ms","hash":340334832,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":1339392,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-08-22T06:42:31.458276Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":340334832,"revision":605,"compact-revision":-1}
{"level":"info","ts":"2024-08-22T06:42:41.352931Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-08-22T06:42:41.352997Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.59.105:2380"],"advertise-client-urls":["https://192.168.59.105:2379"]}
{"level":"warn","ts":"2024-08-22T06:42:41.353208Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-08-22T06:42:41.353304Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-08-22T06:42:41.389016Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.59.105:2379: use of closed network connection"}
{"level":"warn","ts":"2024-08-22T06:42:41.38929Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.59.105:2379: use of closed network connection"}
{"level":"info","ts":"2024-08-22T06:42:41.391212Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"24d1819b191cc3b5","current-leader-member-id":"24d1819b191cc3b5"}
{"level":"info","ts":"2024-08-22T06:42:41.392646Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.59.105:2380"}
{"level":"info","ts":"2024-08-22T06:42:41.392868Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.59.105:2380"}
{"level":"info","ts":"2024-08-22T06:42:41.392885Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.59.105:2380"],"advertise-client-urls":["https://192.168.59.105:2379"]}


==> etcd [487f1bf369e3] <==
{"level":"warn","ts":"2024-08-22T07:01:14.467825Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-08-22T07:01:14.468314Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.59.105:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.59.105:2380","--initial-cluster=minikube=https://192.168.59.105:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.59.105:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.59.105:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-08-22T07:01:14.468506Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-08-22T07:01:14.468586Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-08-22T07:01:14.46862Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.59.105:2380"]}
{"level":"info","ts":"2024-08-22T07:01:14.468675Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-08-22T07:01:14.473608Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.59.105:2379"]}
{"level":"info","ts":"2024-08-22T07:01:14.474764Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":6,"max-cpu-available":6,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.59.105:2380"],"listen-peer-urls":["https://192.168.59.105:2380"],"advertise-client-urls":["https://192.168.59.105:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.59.105:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-08-22T07:01:14.489263Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"12.754188ms"}
{"level":"info","ts":"2024-08-22T07:01:14.507617Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-08-22T07:01:14.526751Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"5a9a8e760cd75cc6","local-member-id":"24d1819b191cc3b5","commit-index":988}
{"level":"info","ts":"2024-08-22T07:01:14.527387Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 switched to configuration voters=()"}
{"level":"info","ts":"2024-08-22T07:01:14.527425Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 became follower at term 2"}
{"level":"info","ts":"2024-08-22T07:01:14.527441Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 24d1819b191cc3b5 [peers: [], term: 2, commit: 988, applied: 0, lastindex: 988, lastterm: 2]"}
{"level":"warn","ts":"2024-08-22T07:01:14.532926Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-08-22T07:01:14.535033Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":605}
{"level":"info","ts":"2024-08-22T07:01:14.545124Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":855}
{"level":"info","ts":"2024-08-22T07:01:14.547582Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-08-22T07:01:14.553581Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"24d1819b191cc3b5","timeout":"7s"}
{"level":"info","ts":"2024-08-22T07:01:14.554347Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"24d1819b191cc3b5"}
{"level":"info","ts":"2024-08-22T07:01:14.5544Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"24d1819b191cc3b5","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-08-22T07:01:14.554826Z","caller":"etcdserver/server.go:760","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-08-22T07:01:14.556365Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-22T07:01:14.557428Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 switched to configuration voters=(2653044158639162293)"}
{"level":"info","ts":"2024-08-22T07:01:14.557553Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"5a9a8e760cd75cc6","local-member-id":"24d1819b191cc3b5","added-peer-id":"24d1819b191cc3b5","added-peer-peer-urls":["https://192.168.59.105:2380"]}
{"level":"info","ts":"2024-08-22T07:01:14.558134Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"5a9a8e760cd75cc6","local-member-id":"24d1819b191cc3b5","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-22T07:01:14.558329Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-22T07:01:14.556867Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-22T07:01:14.569905Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-22T07:01:14.577407Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-08-22T07:01:14.577922Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"24d1819b191cc3b5","initial-advertise-peer-urls":["https://192.168.59.105:2380"],"listen-peer-urls":["https://192.168.59.105:2380"],"advertise-client-urls":["https://192.168.59.105:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.59.105:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-08-22T07:01:14.578006Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-08-22T07:01:14.578135Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.59.105:2380"}
{"level":"info","ts":"2024-08-22T07:01:14.579275Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.59.105:2380"}
{"level":"info","ts":"2024-08-22T07:01:15.530382Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 is starting a new election at term 2"}
{"level":"info","ts":"2024-08-22T07:01:15.530456Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 became pre-candidate at term 2"}
{"level":"info","ts":"2024-08-22T07:01:15.530499Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 received MsgPreVoteResp from 24d1819b191cc3b5 at term 2"}
{"level":"info","ts":"2024-08-22T07:01:15.530877Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 became candidate at term 3"}
{"level":"info","ts":"2024-08-22T07:01:15.530925Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 received MsgVoteResp from 24d1819b191cc3b5 at term 3"}
{"level":"info","ts":"2024-08-22T07:01:15.531081Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"24d1819b191cc3b5 became leader at term 3"}
{"level":"info","ts":"2024-08-22T07:01:15.531192Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 24d1819b191cc3b5 elected leader 24d1819b191cc3b5 at term 3"}
{"level":"info","ts":"2024-08-22T07:01:15.532079Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"24d1819b191cc3b5","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.59.105:2379]}","request-path":"/0/members/24d1819b191cc3b5/attributes","cluster-id":"5a9a8e760cd75cc6","publish-timeout":"7s"}
{"level":"info","ts":"2024-08-22T07:01:15.532448Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-22T07:01:15.532418Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-22T07:01:15.533058Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-08-22T07:01:15.533088Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-08-22T07:01:15.541666Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.59.105:2379"}
{"level":"info","ts":"2024-08-22T07:01:15.55732Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-08-22T07:11:15.636676Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1176}
{"level":"info","ts":"2024-08-22T07:11:15.645115Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1176,"took":"7.196559ms","hash":3015126455,"current-db-size-bytes":2236416,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":2236416,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2024-08-22T07:11:15.645264Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3015126455,"revision":1176,"compact-revision":605}


==> kernel <==
 07:14:06 up 13 min,  0 users,  load average: 0.66, 0.62, 0.44
Linux minikube 5.10.207 #1 SMP Thu May 9 02:07:35 UTC 2024 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2023.02.9"


==> kube-apiserver [2d209653031f] <==
W0822 06:42:46.918996       1 logging.go:59] [core] [Channel #15 SubChannel #16] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:46.936154       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:46.959948       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:46.965648       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:46.970347       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:47.035707       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:47.035748       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:47.082502       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:47.082502       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:47.101640       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:47.139778       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:47.171908       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:49.507753       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:49.598436       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:49.663777       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:49.722832       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:49.731404       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:49.755495       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:49.874932       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.038772       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.071963       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.096681       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.133281       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.135248       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.153779       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.162402       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.176661       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.243900       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.297777       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.345983       1 logging.go:59] [core] [Channel #6 SubChannel #7] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.359342       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.438196       1 logging.go:59] [core] [Channel #15 SubChannel #16] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.460838       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.532973       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.569730       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.650040       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.672534       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.697342       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.730760       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.735576       1 logging.go:59] [core] [Channel #2 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.745338       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.751659       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.769497       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.813882       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.840906       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.843494       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.901205       1 logging.go:59] [core] [Channel #1 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.905878       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.942495       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.983822       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:50.983971       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:51.042868       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:51.099784       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:51.125540       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:51.138528       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:51.139693       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:51.155187       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:51.215966       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:51.348053       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0822 06:42:51.351477       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [feaa13ef46a5] <==
I0822 07:01:17.204439       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0822 07:01:17.204800       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0822 07:01:17.205024       1 secure_serving.go:213] Serving securely on [::]:8443
I0822 07:01:17.205090       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0822 07:01:17.205236       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0822 07:01:17.205335       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0822 07:01:17.205460       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0822 07:01:17.205972       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0822 07:01:17.205311       1 controller.go:78] Starting OpenAPI AggregationController
I0822 07:01:17.206072       1 available_controller.go:423] Starting AvailableConditionController
I0822 07:01:17.206599       1 controller.go:87] Starting OpenAPI V3 controller
I0822 07:01:17.206702       1 controller.go:116] Starting legacy_token_tracking_controller
I0822 07:01:17.206812       1 aggregator.go:163] waiting for initial CRD sync...
I0822 07:01:17.206815       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0822 07:01:17.206869       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0822 07:01:17.207081       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0822 07:01:17.207351       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0822 07:01:17.207378       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0822 07:01:17.207392       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0822 07:01:17.206608       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0822 07:01:17.207350       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0822 07:01:17.206497       1 controller.go:139] Starting OpenAPI controller
I0822 07:01:17.206472       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0822 07:01:17.207365       1 crd_finalizer.go:266] Starting CRDFinalizer
I0822 07:01:17.207375       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0822 07:01:17.206717       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0822 07:01:17.206490       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0822 07:01:17.207931       1 naming_controller.go:291] Starting NamingConditionController
I0822 07:01:17.208077       1 establishing_controller.go:76] Starting EstablishingController
I0822 07:01:17.208152       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0822 07:01:17.207379       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0822 07:01:17.293111       1 shared_informer.go:320] Caches are synced for node_authorizer
I0822 07:01:17.300692       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0822 07:01:17.302172       1 policy_source.go:224] refreshing policies
I0822 07:01:17.305975       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0822 07:01:17.306435       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0822 07:01:17.306473       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0822 07:01:17.306973       1 shared_informer.go:320] Caches are synced for configmaps
I0822 07:01:17.307511       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0822 07:01:17.307581       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0822 07:01:17.307811       1 aggregator.go:165] initial CRD sync complete...
I0822 07:01:17.307842       1 autoregister_controller.go:141] Starting autoregister controller
I0822 07:01:17.307850       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0822 07:01:17.307857       1 cache.go:39] Caches are synced for autoregister controller
I0822 07:01:17.308722       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0822 07:01:17.314550       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0822 07:01:17.342649       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0822 07:01:18.217609       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0822 07:01:18.725213       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.59.105]
I0822 07:01:18.729116       1 controller.go:615] quota admission added evaluator for: endpoints
I0822 07:01:19.289827       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0822 07:01:19.316321       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0822 07:01:19.382570       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0822 07:01:19.446904       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0822 07:01:19.467350       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0822 07:01:30.256160       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0822 07:07:17.272842       1 controller.go:615] quota admission added evaluator for: namespaces
I0822 07:07:17.336852       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0822 07:07:17.505418       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs={"IPv4":"10.101.187.85"}
I0822 07:07:17.528223       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs={"IPv4":"10.107.239.11"}


==> kube-controller-manager [d4d80f7bf621] <==
I0822 06:32:50.670757       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0822 06:32:50.671393       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0822 06:32:50.671641       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0822 06:32:50.671725       1 shared_informer.go:320] Caches are synced for cidrallocator
I0822 06:32:50.675563       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0822 06:32:50.680566       1 range_allocator.go:381] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0822 06:32:50.680881       1 shared_informer.go:320] Caches are synced for TTL
I0822 06:32:50.684587       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0822 06:32:50.693559       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0822 06:32:50.693648       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0822 06:32:50.694147       1 shared_informer.go:320] Caches are synced for HPA
I0822 06:32:50.695249       1 shared_informer.go:320] Caches are synced for PVC protection
I0822 06:32:50.695438       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0822 06:32:50.695469       1 shared_informer.go:320] Caches are synced for PV protection
I0822 06:32:50.696889       1 shared_informer.go:320] Caches are synced for service account
I0822 06:32:50.697033       1 shared_informer.go:320] Caches are synced for TTL after finished
I0822 06:32:50.697167       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0822 06:32:50.697223       1 shared_informer.go:320] Caches are synced for GC
I0822 06:32:50.698404       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0822 06:32:50.698916       1 shared_informer.go:320] Caches are synced for job
I0822 06:32:50.707226       1 shared_informer.go:320] Caches are synced for namespace
I0822 06:32:50.736517       1 shared_informer.go:320] Caches are synced for cronjob
I0822 06:32:50.742733       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0822 06:32:50.745292       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0822 06:32:50.745502       1 shared_informer.go:320] Caches are synced for crt configmap
I0822 06:32:50.745832       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0822 06:32:50.746281       1 shared_informer.go:320] Caches are synced for ReplicationController
I0822 06:32:50.746343       1 shared_informer.go:320] Caches are synced for deployment
I0822 06:32:50.746469       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0822 06:32:50.746981       1 shared_informer.go:320] Caches are synced for persistent volume
I0822 06:32:50.747985       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0822 06:32:50.748254       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0822 06:32:50.748429       1 shared_informer.go:320] Caches are synced for expand
I0822 06:32:50.749551       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0822 06:32:50.750790       1 shared_informer.go:320] Caches are synced for ephemeral
I0822 06:32:50.767368       1 shared_informer.go:320] Caches are synced for endpoint
I0822 06:32:50.793789       1 shared_informer.go:320] Caches are synced for disruption
I0822 06:32:50.845895       1 shared_informer.go:320] Caches are synced for attach detach
I0822 06:32:50.908796       1 shared_informer.go:320] Caches are synced for stateful set
I0822 06:32:50.921190       1 shared_informer.go:320] Caches are synced for daemon sets
I0822 06:32:50.943235       1 shared_informer.go:320] Caches are synced for taint
I0822 06:32:50.943804       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0822 06:32:50.944342       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0822 06:32:50.944720       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0822 06:32:50.947845       1 shared_informer.go:320] Caches are synced for resource quota
I0822 06:32:50.958404       1 shared_informer.go:320] Caches are synced for resource quota
I0822 06:32:51.377248       1 shared_informer.go:320] Caches are synced for garbage collector
I0822 06:32:51.406616       1 shared_informer.go:320] Caches are synced for garbage collector
I0822 06:32:51.406815       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0822 06:32:51.885386       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="275.763925ms"
I0822 06:32:51.899466       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="13.970694ms"
I0822 06:32:51.899726       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="142.759¬µs"
I0822 06:32:51.899851       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="40.439¬µs"
I0822 06:32:51.922562       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="71.537¬µs"
I0822 06:32:53.986629       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="127.528¬µs"
I0822 06:32:54.021516       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="14.495132ms"
I0822 06:32:54.021707       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="117.834¬µs"
I0822 06:32:54.026980       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="76.755¬µs"
I0822 06:32:55.053511       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="14.917368ms"
I0822 06:32:55.056742       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="79.486¬µs"


==> kube-controller-manager [e8745f9c8c4a] <==
I0822 07:01:29.920825       1 shared_informer.go:320] Caches are synced for node
I0822 07:01:29.921338       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0822 07:01:29.921507       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0822 07:01:29.921641       1 shared_informer.go:320] Caches are synced for namespace
I0822 07:01:29.922015       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0822 07:01:29.922540       1 shared_informer.go:320] Caches are synced for cidrallocator
I0822 07:01:29.928620       1 shared_informer.go:320] Caches are synced for ephemeral
I0822 07:01:29.930294       1 shared_informer.go:320] Caches are synced for TTL after finished
I0822 07:01:29.931933       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0822 07:01:29.932021       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0822 07:01:29.932102       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0822 07:01:29.932266       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0822 07:01:29.938781       1 shared_informer.go:320] Caches are synced for taint
I0822 07:01:29.939081       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0822 07:01:29.939233       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0822 07:01:29.939397       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0822 07:01:29.940978       1 shared_informer.go:320] Caches are synced for service account
I0822 07:01:29.945663       1 shared_informer.go:320] Caches are synced for daemon sets
I0822 07:01:29.952259       1 shared_informer.go:320] Caches are synced for job
I0822 07:01:29.952692       1 shared_informer.go:320] Caches are synced for PV protection
I0822 07:01:29.953017       1 shared_informer.go:320] Caches are synced for persistent volume
I0822 07:01:29.957239       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0822 07:01:29.957599       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7c445c467" duration="232.746¬µs"
I0822 07:01:29.958217       1 shared_informer.go:320] Caches are synced for attach detach
I0822 07:01:29.962551       1 shared_informer.go:320] Caches are synced for PVC protection
I0822 07:01:29.965857       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0822 07:01:29.967336       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0822 07:01:29.972621       1 shared_informer.go:320] Caches are synced for GC
I0822 07:01:29.973896       1 shared_informer.go:320] Caches are synced for deployment
I0822 07:01:30.036945       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0822 07:01:30.061081       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0822 07:01:30.154184       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0822 07:01:30.163204       1 shared_informer.go:320] Caches are synced for resource quota
I0822 07:01:30.194830       1 shared_informer.go:320] Caches are synced for resource quota
I0822 07:01:30.588033       1 shared_informer.go:320] Caches are synced for garbage collector
I0822 07:01:30.608526       1 shared_informer.go:320] Caches are synced for garbage collector
I0822 07:01:30.608652       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0822 07:07:17.382578       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="40.001205ms"
E0822 07:07:17.382643       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" failed with pods "dashboard-metrics-scraper-b5fc48f67-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0822 07:07:17.389038       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="25.125064ms"
E0822 07:07:17.389298       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-779776cb65" failed with pods "kubernetes-dashboard-779776cb65-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0822 07:07:17.395294       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="12.083728ms"
E0822 07:07:17.395376       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" failed with pods "dashboard-metrics-scraper-b5fc48f67-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0822 07:07:17.405169       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="9.733929ms"
E0822 07:07:17.405238       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" failed with pods "dashboard-metrics-scraper-b5fc48f67-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0822 07:07:17.405169       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="15.81785ms"
E0822 07:07:17.405314       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-779776cb65" failed with pods "kubernetes-dashboard-779776cb65-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0822 07:07:17.413174       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="7.572619ms"
E0822 07:07:17.413235       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" failed with pods "dashboard-metrics-scraper-b5fc48f67-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0822 07:07:17.437652       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="30.964396ms"
I0822 07:07:17.457446       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="19.678669ms"
I0822 07:07:17.457558       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="37.443¬µs"
I0822 07:07:17.482856       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="29.462419ms"
I0822 07:07:17.497224       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="14.272845ms"
I0822 07:07:17.522181       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="24.799305ms"
I0822 07:07:17.522986       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="721.696¬µs"
I0822 07:10:22.822797       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="174.397¬µs"
I0822 07:10:38.411694       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="326.712¬µs"
I0822 07:13:22.329735       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="77.207¬µs"
I0822 07:13:34.410985       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="170.092¬µs"


==> kube-proxy [2c39b6cf5b7d] <==
I0822 07:01:18.347692       1 server_linux.go:69] "Using iptables proxy"
I0822 07:01:18.380596       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.59.105"]
I0822 07:01:18.474565       1 server_linux.go:143] "No iptables support for family" ipFamily="IPv6"
I0822 07:01:18.474680       1 server.go:661] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0822 07:01:18.474723       1 server_linux.go:165] "Using iptables Proxier"
I0822 07:01:18.479877       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0822 07:01:18.480602       1 server.go:872] "Version info" version="v1.30.0"
I0822 07:01:18.480694       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0822 07:01:18.483023       1 config.go:192] "Starting service config controller"
I0822 07:01:18.483323       1 config.go:101] "Starting endpoint slice config controller"
I0822 07:01:18.483498       1 config.go:319] "Starting node config controller"
I0822 07:01:18.483391       1 shared_informer.go:313] Waiting for caches to sync for service config
I0822 07:01:18.483452       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0822 07:01:18.483521       1 shared_informer.go:313] Waiting for caches to sync for node config
I0822 07:01:18.584531       1 shared_informer.go:320] Caches are synced for node config
I0822 07:01:18.584609       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0822 07:01:18.584610       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [623115b7506c] <==
I0822 06:32:52.640797       1 server_linux.go:69] "Using iptables proxy"
I0822 06:32:52.653524       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.59.105"]
I0822 06:32:52.704237       1 server_linux.go:143] "No iptables support for family" ipFamily="IPv6"
I0822 06:32:52.704323       1 server.go:661] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0822 06:32:52.704348       1 server_linux.go:165] "Using iptables Proxier"
I0822 06:32:52.707548       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0822 06:32:52.707879       1 server.go:872] "Version info" version="v1.30.0"
I0822 06:32:52.707911       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0822 06:32:52.709458       1 config.go:192] "Starting service config controller"
I0822 06:32:52.709488       1 shared_informer.go:313] Waiting for caches to sync for service config
I0822 06:32:52.709585       1 config.go:319] "Starting node config controller"
I0822 06:32:52.709623       1 shared_informer.go:313] Waiting for caches to sync for node config
I0822 06:32:52.709578       1 config.go:101] "Starting endpoint slice config controller"
I0822 06:32:52.709665       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0822 06:32:52.814405       1 shared_informer.go:320] Caches are synced for node config
I0822 06:32:52.814559       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0822 06:32:52.814917       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [6b5386fc8173] <==
W0822 06:32:33.157503       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0822 06:32:33.157459       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0822 06:32:33.157701       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0822 06:32:33.157876       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0822 06:32:33.157979       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0822 06:32:33.158236       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0822 06:32:33.157911       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0822 06:32:33.158028       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0822 06:32:33.157653       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0822 06:32:33.158400       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0822 06:32:33.157475       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0822 06:32:33.158540       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0822 06:32:33.158919       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0822 06:32:33.158976       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0822 06:32:33.158925       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0822 06:32:33.159026       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0822 06:32:33.158938       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0822 06:32:33.159040       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0822 06:32:33.159159       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0822 06:32:33.159166       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0822 06:32:33.159333       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0822 06:32:33.159408       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0822 06:32:33.159630       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0822 06:32:33.159717       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0822 06:32:33.159748       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0822 06:32:33.159902       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0822 06:32:33.996597       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0822 06:32:33.996863       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0822 06:32:34.079076       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0822 06:32:34.079354       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0822 06:32:34.092773       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0822 06:32:34.092859       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0822 06:32:34.176536       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0822 06:32:34.176590       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0822 06:32:34.212477       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0822 06:32:34.212538       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0822 06:32:34.274045       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0822 06:32:34.274227       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0822 06:32:34.406637       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0822 06:32:34.406831       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0822 06:32:34.428315       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0822 06:32:34.428389       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0822 06:32:34.464258       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0822 06:32:34.464421       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0822 06:32:34.464501       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0822 06:32:34.464440       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0822 06:32:34.484528       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0822 06:32:34.484643       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0822 06:32:34.515742       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0822 06:32:34.515900       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0822 06:32:34.743654       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0822 06:32:34.744372       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0822 06:32:34.754403       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0822 06:32:34.754463       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0822 06:32:34.764960       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0822 06:32:34.765042       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
I0822 06:32:36.954682       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0822 06:42:41.363693       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0822 06:42:41.363883       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E0822 06:42:41.364085       1 run.go:74] "command failed" err="finished without leader elect"


==> kube-scheduler [f5fea639ca6e] <==
I0822 07:01:15.203960       1 serving.go:380] Generated self-signed cert in-memory
W0822 07:01:17.249935       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0822 07:01:17.250052       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0822 07:01:17.250075       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0822 07:01:17.250085       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0822 07:01:17.287152       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0822 07:01:17.287205       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0822 07:01:17.290504       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0822 07:01:17.290553       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0822 07:01:17.291931       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0822 07:01:17.292302       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0822 07:01:17.392810       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Aug 22 07:05:13 minikube kubelet[1526]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 22 07:05:13 minikube kubelet[1526]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Aug 22 07:06:13 minikube kubelet[1526]: E0822 07:06:13.422393    1526 iptables.go:577] "Could not set up iptables canary" err=<
Aug 22 07:06:13 minikube kubelet[1526]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Aug 22 07:06:13 minikube kubelet[1526]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 22 07:06:13 minikube kubelet[1526]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 22 07:06:13 minikube kubelet[1526]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Aug 22 07:07:13 minikube kubelet[1526]: E0822 07:07:13.424400    1526 iptables.go:577] "Could not set up iptables canary" err=<
Aug 22 07:07:13 minikube kubelet[1526]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Aug 22 07:07:13 minikube kubelet[1526]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 22 07:07:13 minikube kubelet[1526]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 22 07:07:13 minikube kubelet[1526]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Aug 22 07:07:17 minikube kubelet[1526]: I0822 07:07:17.431574    1526 topology_manager.go:215] "Topology Admit Handler" podUID="84219176-0de1-4050-8397-c6ddd702f67a" podNamespace="kubernetes-dashboard" podName="kubernetes-dashboard-779776cb65-wctlm"
Aug 22 07:07:17 minikube kubelet[1526]: I0822 07:07:17.472388    1526 topology_manager.go:215] "Topology Admit Handler" podUID="bf008351-0efb-4bea-9809-b7388884a9b3" podNamespace="kubernetes-dashboard" podName="dashboard-metrics-scraper-b5fc48f67-h6pzr"
Aug 22 07:07:17 minikube kubelet[1526]: I0822 07:07:17.543007    1526 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/bf008351-0efb-4bea-9809-b7388884a9b3-tmp-volume\") pod \"dashboard-metrics-scraper-b5fc48f67-h6pzr\" (UID: \"bf008351-0efb-4bea-9809-b7388884a9b3\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-h6pzr"
Aug 22 07:07:17 minikube kubelet[1526]: I0822 07:07:17.543100    1526 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-p4wtw\" (UniqueName: \"kubernetes.io/projected/bf008351-0efb-4bea-9809-b7388884a9b3-kube-api-access-p4wtw\") pod \"dashboard-metrics-scraper-b5fc48f67-h6pzr\" (UID: \"bf008351-0efb-4bea-9809-b7388884a9b3\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-h6pzr"
Aug 22 07:07:17 minikube kubelet[1526]: I0822 07:07:17.543147    1526 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/84219176-0de1-4050-8397-c6ddd702f67a-tmp-volume\") pod \"kubernetes-dashboard-779776cb65-wctlm\" (UID: \"84219176-0de1-4050-8397-c6ddd702f67a\") " pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-wctlm"
Aug 22 07:07:17 minikube kubelet[1526]: I0822 07:07:17.543177    1526 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tdnmb\" (UniqueName: \"kubernetes.io/projected/84219176-0de1-4050-8397-c6ddd702f67a-kube-api-access-tdnmb\") pod \"kubernetes-dashboard-779776cb65-wctlm\" (UID: \"84219176-0de1-4050-8397-c6ddd702f67a\") " pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-wctlm"
Aug 22 07:07:18 minikube kubelet[1526]: I0822 07:07:18.039568    1526 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="da5ed1bd4dbbac2586f5a6a5f6b37a1ffebddeecdf013c4c5402fed885a0bbe9"
Aug 22 07:07:18 minikube kubelet[1526]: I0822 07:07:18.071023    1526 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c860552e81e5494868df67fc019a0801a1413614d55731524a64bdc4ec4507ff"
Aug 22 07:08:13 minikube kubelet[1526]: E0822 07:08:13.418914    1526 iptables.go:577] "Could not set up iptables canary" err=<
Aug 22 07:08:13 minikube kubelet[1526]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Aug 22 07:08:13 minikube kubelet[1526]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 22 07:08:13 minikube kubelet[1526]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 22 07:08:13 minikube kubelet[1526]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Aug 22 07:09:13 minikube kubelet[1526]: E0822 07:09:13.423206    1526 iptables.go:577] "Could not set up iptables canary" err=<
Aug 22 07:09:13 minikube kubelet[1526]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Aug 22 07:09:13 minikube kubelet[1526]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 22 07:09:13 minikube kubelet[1526]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 22 07:09:13 minikube kubelet[1526]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Aug 22 07:10:13 minikube kubelet[1526]: E0822 07:10:13.421570    1526 iptables.go:577] "Could not set up iptables canary" err=<
Aug 22 07:10:13 minikube kubelet[1526]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Aug 22 07:10:13 minikube kubelet[1526]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 22 07:10:13 minikube kubelet[1526]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 22 07:10:13 minikube kubelet[1526]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Aug 22 07:10:22 minikube kubelet[1526]: E0822 07:10:22.341936    1526 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = error pulling image configuration: download failed after attempts=6: dial tcp 59.188.250.54:443: i/o timeout" image="docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Aug 22 07:10:22 minikube kubelet[1526]: E0822 07:10:22.342084    1526 kuberuntime_image.go:55] "Failed to pull image" err="error pulling image configuration: download failed after attempts=6: dial tcp 59.188.250.54:443: i/o timeout" image="docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Aug 22 07:10:22 minikube kubelet[1526]: E0822 07:10:22.343588    1526 kuberuntime_manager.go:1256] container &Container{Name:kubernetes-dashboard,Image:docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93,Command:[],Args:[--namespace=kubernetes-dashboard --enable-skip-login --disable-settings-authorizer],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9090,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp-volume,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-tdnmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/,Port:{0 9090 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:30,TimeoutSeconds:30,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*1001,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:*2001,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod kubernetes-dashboard-779776cb65-wctlm_kubernetes-dashboard(84219176-0de1-4050-8397-c6ddd702f67a): ErrImagePull: error pulling image configuration: download failed after attempts=6: dial tcp 59.188.250.54:443: i/o timeout
Aug 22 07:10:22 minikube kubelet[1526]: E0822 07:10:22.343756    1526 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ErrImagePull: \"error pulling image configuration: download failed after attempts=6: dial tcp 59.188.250.54:443: i/o timeout\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-wctlm" podUID="84219176-0de1-4050-8397-c6ddd702f67a"
Aug 22 07:10:22 minikube kubelet[1526]: E0822 07:10:22.808939    1526 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\"\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-wctlm" podUID="84219176-0de1-4050-8397-c6ddd702f67a"
Aug 22 07:11:13 minikube kubelet[1526]: E0822 07:11:13.422343    1526 iptables.go:577] "Could not set up iptables canary" err=<
Aug 22 07:11:13 minikube kubelet[1526]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Aug 22 07:11:13 minikube kubelet[1526]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 22 07:11:13 minikube kubelet[1526]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 22 07:11:13 minikube kubelet[1526]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Aug 22 07:12:13 minikube kubelet[1526]: E0822 07:12:13.420041    1526 iptables.go:577] "Could not set up iptables canary" err=<
Aug 22 07:12:13 minikube kubelet[1526]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Aug 22 07:12:13 minikube kubelet[1526]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 22 07:12:13 minikube kubelet[1526]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 22 07:12:13 minikube kubelet[1526]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Aug 22 07:13:13 minikube kubelet[1526]: E0822 07:13:13.423970    1526 iptables.go:577] "Could not set up iptables canary" err=<
Aug 22 07:13:13 minikube kubelet[1526]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Aug 22 07:13:13 minikube kubelet[1526]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Aug 22 07:13:13 minikube kubelet[1526]:         Perhaps ip6tables or your kernel needs to be upgraded.
Aug 22 07:13:13 minikube kubelet[1526]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Aug 22 07:13:21 minikube kubelet[1526]: E0822 07:13:21.412610    1526 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = error pulling image configuration: download failed after attempts=6: dial tcp 174.37.154.236:443: i/o timeout" image="docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Aug 22 07:13:21 minikube kubelet[1526]: E0822 07:13:21.412691    1526 kuberuntime_image.go:55] "Failed to pull image" err="error pulling image configuration: download failed after attempts=6: dial tcp 174.37.154.236:443: i/o timeout" image="docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Aug 22 07:13:21 minikube kubelet[1526]: E0822 07:13:21.413400    1526 kuberuntime_manager.go:1256] container &Container{Name:dashboard-metrics-scraper,Image:docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp-volume,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-p4wtw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/,Port:{0 8000 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:30,TimeoutSeconds:30,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*1001,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:*2001,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod dashboard-metrics-scraper-b5fc48f67-h6pzr_kubernetes-dashboard(bf008351-0efb-4bea-9809-b7388884a9b3): ErrImagePull: error pulling image configuration: download failed after attempts=6: dial tcp 174.37.154.236:443: i/o timeout
Aug 22 07:13:21 minikube kubelet[1526]: E0822 07:13:21.413448    1526 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ErrImagePull: \"error pulling image configuration: download failed after attempts=6: dial tcp 174.37.154.236:443: i/o timeout\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-h6pzr" podUID="bf008351-0efb-4bea-9809-b7388884a9b3"
Aug 22 07:13:22 minikube kubelet[1526]: E0822 07:13:22.315112    1526 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\"\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-h6pzr" podUID="bf008351-0efb-4bea-9809-b7388884a9b3"


==> storage-provisioner [1074a830568c] <==
I0822 07:02:00.588085       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0822 07:02:00.612258       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0822 07:02:00.613172       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0822 07:02:18.045983       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0822 07:02:18.046530       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_8a23797b-4cb0-4d4d-95e4-51c88f3b384c!
I0822 07:02:18.046526       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"febd6414-baa2-4160-943f-d6421e867c9a", APIVersion:"v1", ResourceVersion:"986", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_8a23797b-4cb0-4d4d-95e4-51c88f3b384c became leader
I0822 07:02:18.147885       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_8a23797b-4cb0-4d4d-95e4-51c88f3b384c!


==> storage-provisioner [66b9676eddf9] <==
I0822 07:01:18.282516       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0822 07:01:48.293489       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

